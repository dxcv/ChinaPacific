{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US NCREIF: FUND INDEX OPEN-END DIVERSIFIED CORE RETURNS NADJ</th>\n",
       "      <th>US Private Equity</th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "      <th>Cash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.018311</td>\n",
       "      <td>0.034503</td>\n",
       "      <td>0.018989</td>\n",
       "      <td>0.035495</td>\n",
       "      <td>0.024647</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>0.028452</td>\n",
       "      <td>0.017290</td>\n",
       "      <td>0.032061</td>\n",
       "      <td>0.008155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029578</td>\n",
       "      <td>0.047497</td>\n",
       "      <td>0.047757</td>\n",
       "      <td>0.070263</td>\n",
       "      <td>0.064625</td>\n",
       "      <td>0.076253</td>\n",
       "      <td>0.099660</td>\n",
       "      <td>0.091792</td>\n",
       "      <td>0.128686</td>\n",
       "      <td>0.006670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.136900</td>\n",
       "      <td>-0.159500</td>\n",
       "      <td>-0.333429</td>\n",
       "      <td>-0.214401</td>\n",
       "      <td>-0.215870</td>\n",
       "      <td>-0.219432</td>\n",
       "      <td>-0.261180</td>\n",
       "      <td>-0.211290</td>\n",
       "      <td>-0.275584</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.012925</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.007742</td>\n",
       "      <td>0.004679</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.001749</td>\n",
       "      <td>-0.030336</td>\n",
       "      <td>-0.016548</td>\n",
       "      <td>-0.043980</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.024050</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.018845</td>\n",
       "      <td>0.037985</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>0.031222</td>\n",
       "      <td>0.037918</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.038349</td>\n",
       "      <td>0.008214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.035350</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.035783</td>\n",
       "      <td>0.069509</td>\n",
       "      <td>0.049455</td>\n",
       "      <td>0.070468</td>\n",
       "      <td>0.088459</td>\n",
       "      <td>0.066656</td>\n",
       "      <td>0.111489</td>\n",
       "      <td>0.013101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.054500</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.137135</td>\n",
       "      <td>0.264363</td>\n",
       "      <td>0.427906</td>\n",
       "      <td>0.212974</td>\n",
       "      <td>0.297346</td>\n",
       "      <td>0.258489</td>\n",
       "      <td>0.348433</td>\n",
       "      <td>0.023825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       US NCREIF: FUND INDEX OPEN-END DIVERSIFIED CORE RETURNS NADJ  \\\n",
       "count                                         114.000000              \n",
       "mean                                            0.018311              \n",
       "std                                             0.029578              \n",
       "min                                            -0.136900              \n",
       "25%                                             0.012925              \n",
       "50%                                             0.024050              \n",
       "75%                                             0.035350              \n",
       "max                                             0.054500              \n",
       "\n",
       "       US Private Equity       US_RE       US_PE       US_HY       SP500  \\\n",
       "count         114.000000  114.000000  114.000000  114.000000  114.000000   \n",
       "mean            0.034503    0.018989    0.035495    0.024647    0.027670   \n",
       "std             0.047497    0.047757    0.070263    0.064625    0.076253   \n",
       "min            -0.159500   -0.333429   -0.214401   -0.215870   -0.219432   \n",
       "25%             0.011300    0.007742    0.004679   -0.002124   -0.001749   \n",
       "50%             0.037200    0.018845    0.037985    0.026129    0.031222   \n",
       "75%             0.059000    0.035783    0.069509    0.049455    0.070468   \n",
       "max             0.178000    0.137135    0.264363    0.427906    0.212974   \n",
       "\n",
       "       Rusell2000        EAFE          EM        Cash  \n",
       "count  114.000000  114.000000  114.000000  114.000000  \n",
       "mean     0.028452    0.017290    0.032061    0.008155  \n",
       "std      0.099660    0.091792    0.128686    0.006670  \n",
       "min     -0.261180   -0.211290   -0.275584    0.000000  \n",
       "25%     -0.030336   -0.016548   -0.043980    0.000726  \n",
       "50%      0.037918    0.017968    0.038349    0.008214  \n",
       "75%      0.088459    0.066656    0.111489    0.013101  \n",
       "max      0.297346    0.258489    0.348433    0.023825  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read in data\n",
    "ret_df_raw= pd.read_excel( io= 'Data/cipc data_20170907.xlsx', sheetname= 'Data_Input', index_col=0)\n",
    "ret_df_raw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ret_df= ret_df_raw[['US_RE', \n",
    "                   'US_PE',\n",
    "                   'US_HY',\n",
    "                   'SP500',\n",
    "                   'Rusell2000',\n",
    "                   'EAFE',\n",
    "                   'EM']]\n",
    "ret_df_cov= ret_df.cov()\n",
    "ret_df_corr= ret_df.corr()\n",
    "N= ret_df.shape[1]\n",
    "#ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# period1_start= '1987-01-01'\n",
    "# period1_end= '1999-07-01'\n",
    "# period2_start= '2002-01-01'\n",
    "# period2_end= '2008-01-01'\n",
    "# period3_start= '2009-12-31'\n",
    "# period3_end= '2017-12-31'\n",
    "\n",
    "# ret_df1= ret_df[ np.logical_and( ret_df.index>= period1_start, ret_df.index<= period1_end) ]\n",
    "# ret_df2= ret_df[ np.logical_and( ret_df.index>= period2_start, ret_df.index<= period2_end) ]\n",
    "# ret_df3= ret_df[ np.logical_and( ret_df.index>= period3_start, ret_df.index<= period3_end) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.003805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.006220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.004692</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>0.001175</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.007698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.003805</td>\n",
       "      <td>0.006220</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.015923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "US_RE       0.002540  0.001038  0.000806  0.001162    0.001527  0.001175   \n",
       "US_PE       0.001038  0.005029  0.001758  0.003705    0.004692  0.003906   \n",
       "US_HY       0.000806  0.001758  0.004316  0.002500    0.003166  0.002726   \n",
       "SP500       0.001162  0.003705  0.002500  0.005852    0.006276  0.005322   \n",
       "Rusell2000  0.001527  0.004692  0.003166  0.006276    0.009711  0.006270   \n",
       "EAFE        0.001175  0.003906  0.002726  0.005322    0.006270  0.008299   \n",
       "EM          0.001462  0.005121  0.003805  0.006220    0.008594  0.007698   \n",
       "\n",
       "                  EM  \n",
       "US_RE       0.001462  \n",
       "US_PE       0.005121  \n",
       "US_HY       0.003805  \n",
       "SP500       0.006220  \n",
       "Rusell2000  0.008594  \n",
       "EAFE        0.007698  \n",
       "EM          0.015923  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ledoit Wolf shrunk cov matrix\n",
    "\n",
    "\n",
    "from sklearn.covariance import LedoitWolf\n",
    "LW= LedoitWolf( ).fit(ret_df)\n",
    "LW_alpha= LW.shrinkage_\n",
    "\n",
    "LW_cov= pd.DataFrame(LW.covariance_)\n",
    "LW_cov.index= ret_df_cov.index\n",
    "LW_cov.columns= ret_df_cov.columns\n",
    "LW_cov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10078878,  0.14183195,  0.13139679,  0.15299213,  0.19708686,\n",
       "        0.18219804,  0.25237214])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.diagonal(np.matrix(LW_cov.values)))*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## arithmatic avg ret to exponential avg ret \n",
    "\n",
    "ret_cov= np.diagonal(np.matrix(LW_cov.values))\n",
    "coverter= np.array([ret_cov.tolist()]*ret_df.shape[0])* .5\n",
    "ret_df_exp= ret_df- coverter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_eq= np.ones( (7,))*1.0/7\n",
    "weight_peer= np.array( (0.14,0.29,0.05,0.24,0.03,0.21,0.05))\n",
    "weight_peer= weight_peer/ np.sum(weight_peer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 11\n",
      "         Function evaluations: 1249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\X197066\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:394: RuntimeWarning: Method Powell cannot handle constraints nor bounds.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "## solve ERC weight \n",
    "\n",
    "def objective_func(w, sigma): \n",
    "    A= np.diag( w)\n",
    "    B= np.diag( np.dot( sigma, w))\n",
    "    C= np.diag( np.dot( A, B))/ np.dot( np.dot( w, sigma), w)- np.ones( w.size )* 1/ w.size\n",
    "    \n",
    "    return np.dot( C, C)\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize \n",
    "\n",
    "opt_res= minimize( objective_func, \n",
    "                 x0= weight_eq,\n",
    "                 args= LW_cov,\n",
    "                 method= 'Powell',\n",
    "                 options= {'disp': True},\n",
    "                 bounds= [[0,None]]*7,\n",
    "                 tol= 1e-16)\n",
    "\n",
    "weight_erc = opt_res.x/ np.sum( opt_res.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7843033661878602e-23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objective_func( weight_erc, LW_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_eq</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_peer</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_erc</th>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.142979</td>\n",
       "      <td>0.177851</td>\n",
       "      <td>0.118734</td>\n",
       "      <td>0.092724</td>\n",
       "      <td>0.105874</td>\n",
       "      <td>0.079423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "weight_eq    0.142857  0.142857  0.142857  0.142857    0.142857  0.142857   \n",
       "weight_peer  0.138614  0.287129  0.049505  0.237624    0.029703  0.207921   \n",
       "weight_erc   0.282415  0.142979  0.177851  0.118734    0.092724  0.105874   \n",
       "\n",
       "                   EM  \n",
       "weight_eq    0.142857  \n",
       "weight_peer  0.049505  \n",
       "weight_erc   0.079423  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "portf_weight_1= pd.DataFrame( [weight_eq, weight_peer, weight_erc], \n",
    "                             index=['weight_eq', 'weight_peer', 'weight_erc'], \n",
    "                             columns= LW_cov. columns)\n",
    "portf_weight_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## recover the implied expected ret based on shirinked cov matrix\n",
    "\n",
    "rf= 179/10000\n",
    "gamma= [ 1.5, 2, 2.5, 3, 3.5,4]\n",
    "implied_ExpRet= {}\n",
    "\n",
    "for w_name in portf_weight_1.index: \n",
    "    tmp_dic= {}\n",
    "    for g in gamma:\n",
    "        w= np.array(portf_weight_1.loc[w_name].tolist())\n",
    "        tmp1= np.ones( ( N))* rf/4+ g*  np.dot( LW_cov, w)\n",
    "        tmp2= np.ones( (N))*rf/4+ g* np.dot( ret_df_cov,w) \n",
    "        tmp_dic[str(g)+ '_shrunk']= tmp1\n",
    "        tmp_dic[str(g)+'_unshrunk']= tmp2\n",
    "    \n",
    "    \n",
    "    tmp= pd.DataFrame( tmp_dic, index= LW_cov.index)\n",
    "    tmp= tmp- .5* np.array([np.diag(LW_cov).tolist()] *tmp.shape[1]).T\n",
    "    implied_ExpRet[w_name]= tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.5_shrunk</th>\n",
       "      <th>1.5_unshrunk</th>\n",
       "      <th>2.5_shrunk</th>\n",
       "      <th>2.5_unshrunk</th>\n",
       "      <th>2_shrunk</th>\n",
       "      <th>2_unshrunk</th>\n",
       "      <th>3.5_shrunk</th>\n",
       "      <th>3.5_unshrunk</th>\n",
       "      <th>3_shrunk</th>\n",
       "      <th>3_unshrunk</th>\n",
       "      <th>4_shrunk</th>\n",
       "      <th>4_unshrunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>2.114289</td>\n",
       "      <td>2.133245</td>\n",
       "      <td>2.669094</td>\n",
       "      <td>2.700688</td>\n",
       "      <td>2.391691</td>\n",
       "      <td>2.416967</td>\n",
       "      <td>3.223899</td>\n",
       "      <td>3.268131</td>\n",
       "      <td>2.946497</td>\n",
       "      <td>2.984409</td>\n",
       "      <td>3.501302</td>\n",
       "      <td>3.551852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>2.948458</td>\n",
       "      <td>3.056591</td>\n",
       "      <td>4.391306</td>\n",
       "      <td>4.571528</td>\n",
       "      <td>3.669882</td>\n",
       "      <td>3.814060</td>\n",
       "      <td>5.834154</td>\n",
       "      <td>6.086466</td>\n",
       "      <td>5.112730</td>\n",
       "      <td>5.328997</td>\n",
       "      <td>6.555578</td>\n",
       "      <td>6.843935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>2.562022</td>\n",
       "      <td>2.634741</td>\n",
       "      <td>3.652207</td>\n",
       "      <td>3.773405</td>\n",
       "      <td>3.107114</td>\n",
       "      <td>3.204073</td>\n",
       "      <td>4.742391</td>\n",
       "      <td>4.912069</td>\n",
       "      <td>4.197299</td>\n",
       "      <td>4.342737</td>\n",
       "      <td>5.287484</td>\n",
       "      <td>5.481402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>3.279979</td>\n",
       "      <td>3.421321</td>\n",
       "      <td>5.053519</td>\n",
       "      <td>5.289088</td>\n",
       "      <td>4.166749</td>\n",
       "      <td>4.355205</td>\n",
       "      <td>6.827058</td>\n",
       "      <td>7.156855</td>\n",
       "      <td>5.940288</td>\n",
       "      <td>6.222972</td>\n",
       "      <td>7.713828</td>\n",
       "      <td>8.090739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>3.296555</td>\n",
       "      <td>3.490678</td>\n",
       "      <td>5.595700</td>\n",
       "      <td>5.919238</td>\n",
       "      <td>4.446127</td>\n",
       "      <td>4.704958</td>\n",
       "      <td>7.894844</td>\n",
       "      <td>8.347797</td>\n",
       "      <td>6.745272</td>\n",
       "      <td>7.133518</td>\n",
       "      <td>9.044416</td>\n",
       "      <td>9.562077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>3.164197</td>\n",
       "      <td>3.330556</td>\n",
       "      <td>5.186866</td>\n",
       "      <td>5.464131</td>\n",
       "      <td>4.175532</td>\n",
       "      <td>4.397344</td>\n",
       "      <td>7.209535</td>\n",
       "      <td>7.597706</td>\n",
       "      <td>6.198200</td>\n",
       "      <td>6.530919</td>\n",
       "      <td>8.220869</td>\n",
       "      <td>8.664494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>2.790269</td>\n",
       "      <td>3.033674</td>\n",
       "      <td>5.580172</td>\n",
       "      <td>5.985847</td>\n",
       "      <td>4.185221</td>\n",
       "      <td>4.509760</td>\n",
       "      <td>8.370075</td>\n",
       "      <td>8.938019</td>\n",
       "      <td>6.975123</td>\n",
       "      <td>7.461933</td>\n",
       "      <td>9.765026</td>\n",
       "      <td>10.414105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1.5_shrunk  1.5_unshrunk  2.5_shrunk  2.5_unshrunk  2_shrunk  \\\n",
       "US_RE         2.114289      2.133245    2.669094      2.700688  2.391691   \n",
       "US_PE         2.948458      3.056591    4.391306      4.571528  3.669882   \n",
       "US_HY         2.562022      2.634741    3.652207      3.773405  3.107114   \n",
       "SP500         3.279979      3.421321    5.053519      5.289088  4.166749   \n",
       "Rusell2000    3.296555      3.490678    5.595700      5.919238  4.446127   \n",
       "EAFE          3.164197      3.330556    5.186866      5.464131  4.175532   \n",
       "EM            2.790269      3.033674    5.580172      5.985847  4.185221   \n",
       "\n",
       "            2_unshrunk  3.5_shrunk  3.5_unshrunk  3_shrunk  3_unshrunk  \\\n",
       "US_RE         2.416967    3.223899      3.268131  2.946497    2.984409   \n",
       "US_PE         3.814060    5.834154      6.086466  5.112730    5.328997   \n",
       "US_HY         3.204073    4.742391      4.912069  4.197299    4.342737   \n",
       "SP500         4.355205    6.827058      7.156855  5.940288    6.222972   \n",
       "Rusell2000    4.704958    7.894844      8.347797  6.745272    7.133518   \n",
       "EAFE          4.397344    7.209535      7.597706  6.198200    6.530919   \n",
       "EM            4.509760    8.370075      8.938019  6.975123    7.461933   \n",
       "\n",
       "            4_shrunk  4_unshrunk  \n",
       "US_RE       3.501302    3.551852  \n",
       "US_PE       6.555578    6.843935  \n",
       "US_HY       5.287484    5.481402  \n",
       "SP500       7.713828    8.090739  \n",
       "Rusell2000  9.044416    9.562077  \n",
       "EAFE        8.220869    8.664494  \n",
       "EM          9.765026   10.414105  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implied_ExpRet['weight_eq']*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.5_shrunk</th>\n",
       "      <th>1.5_unshrunk</th>\n",
       "      <th>2.5_shrunk</th>\n",
       "      <th>2.5_unshrunk</th>\n",
       "      <th>2_shrunk</th>\n",
       "      <th>2_unshrunk</th>\n",
       "      <th>3.5_shrunk</th>\n",
       "      <th>3.5_unshrunk</th>\n",
       "      <th>3_shrunk</th>\n",
       "      <th>3_unshrunk</th>\n",
       "      <th>4_shrunk</th>\n",
       "      <th>4_unshrunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>2.199490</td>\n",
       "      <td>2.188242</td>\n",
       "      <td>2.811095</td>\n",
       "      <td>2.792348</td>\n",
       "      <td>2.505292</td>\n",
       "      <td>2.490295</td>\n",
       "      <td>3.422701</td>\n",
       "      <td>3.396455</td>\n",
       "      <td>3.116898</td>\n",
       "      <td>3.094402</td>\n",
       "      <td>3.728504</td>\n",
       "      <td>3.698509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>2.596270</td>\n",
       "      <td>2.680794</td>\n",
       "      <td>3.804326</td>\n",
       "      <td>3.945200</td>\n",
       "      <td>3.200298</td>\n",
       "      <td>3.312997</td>\n",
       "      <td>5.012383</td>\n",
       "      <td>5.209607</td>\n",
       "      <td>4.408355</td>\n",
       "      <td>4.577403</td>\n",
       "      <td>5.616411</td>\n",
       "      <td>5.841810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>2.383524</td>\n",
       "      <td>2.435290</td>\n",
       "      <td>3.354711</td>\n",
       "      <td>3.440986</td>\n",
       "      <td>2.869117</td>\n",
       "      <td>2.938138</td>\n",
       "      <td>4.325897</td>\n",
       "      <td>4.446683</td>\n",
       "      <td>3.840304</td>\n",
       "      <td>3.943835</td>\n",
       "      <td>4.811491</td>\n",
       "      <td>4.949532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>2.801773</td>\n",
       "      <td>2.917307</td>\n",
       "      <td>4.256508</td>\n",
       "      <td>4.449065</td>\n",
       "      <td>3.529140</td>\n",
       "      <td>3.683186</td>\n",
       "      <td>5.711243</td>\n",
       "      <td>5.980822</td>\n",
       "      <td>4.983875</td>\n",
       "      <td>5.214944</td>\n",
       "      <td>6.438610</td>\n",
       "      <td>6.746701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>2.642041</td>\n",
       "      <td>2.805246</td>\n",
       "      <td>4.504843</td>\n",
       "      <td>4.776850</td>\n",
       "      <td>3.573442</td>\n",
       "      <td>3.791048</td>\n",
       "      <td>6.367644</td>\n",
       "      <td>6.748455</td>\n",
       "      <td>5.436244</td>\n",
       "      <td>5.762653</td>\n",
       "      <td>7.299045</td>\n",
       "      <td>7.734257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>2.577358</td>\n",
       "      <td>2.713946</td>\n",
       "      <td>4.208801</td>\n",
       "      <td>4.436448</td>\n",
       "      <td>3.393080</td>\n",
       "      <td>3.575197</td>\n",
       "      <td>5.840244</td>\n",
       "      <td>6.158950</td>\n",
       "      <td>5.024522</td>\n",
       "      <td>5.297699</td>\n",
       "      <td>6.655965</td>\n",
       "      <td>7.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>1.867548</td>\n",
       "      <td>2.065501</td>\n",
       "      <td>4.042303</td>\n",
       "      <td>4.372225</td>\n",
       "      <td>2.954925</td>\n",
       "      <td>3.218863</td>\n",
       "      <td>6.217058</td>\n",
       "      <td>6.678948</td>\n",
       "      <td>5.129680</td>\n",
       "      <td>5.525587</td>\n",
       "      <td>7.304435</td>\n",
       "      <td>7.832310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1.5_shrunk  1.5_unshrunk  2.5_shrunk  2.5_unshrunk  2_shrunk  \\\n",
       "US_RE         2.199490      2.188242    2.811095      2.792348  2.505292   \n",
       "US_PE         2.596270      2.680794    3.804326      3.945200  3.200298   \n",
       "US_HY         2.383524      2.435290    3.354711      3.440986  2.869117   \n",
       "SP500         2.801773      2.917307    4.256508      4.449065  3.529140   \n",
       "Rusell2000    2.642041      2.805246    4.504843      4.776850  3.573442   \n",
       "EAFE          2.577358      2.713946    4.208801      4.436448  3.393080   \n",
       "EM            1.867548      2.065501    4.042303      4.372225  2.954925   \n",
       "\n",
       "            2_unshrunk  3.5_shrunk  3.5_unshrunk  3_shrunk  3_unshrunk  \\\n",
       "US_RE         2.490295    3.422701      3.396455  3.116898    3.094402   \n",
       "US_PE         3.312997    5.012383      5.209607  4.408355    4.577403   \n",
       "US_HY         2.938138    4.325897      4.446683  3.840304    3.943835   \n",
       "SP500         3.683186    5.711243      5.980822  4.983875    5.214944   \n",
       "Rusell2000    3.791048    6.367644      6.748455  5.436244    5.762653   \n",
       "EAFE          3.575197    5.840244      6.158950  5.024522    5.297699   \n",
       "EM            3.218863    6.217058      6.678948  5.129680    5.525587   \n",
       "\n",
       "            4_shrunk  4_unshrunk  \n",
       "US_RE       3.728504    3.698509  \n",
       "US_PE       5.616411    5.841810  \n",
       "US_HY       4.811491    4.949532  \n",
       "SP500       6.438610    6.746701  \n",
       "Rusell2000  7.299045    7.734257  \n",
       "EAFE        6.655965    7.020200  \n",
       "EM          7.304435    7.832310  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implied_ExpRet['weight_erc']*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.5_shrunk</th>\n",
       "      <th>1.5_unshrunk</th>\n",
       "      <th>2.5_shrunk</th>\n",
       "      <th>2.5_unshrunk</th>\n",
       "      <th>2_shrunk</th>\n",
       "      <th>2_unshrunk</th>\n",
       "      <th>3.5_shrunk</th>\n",
       "      <th>3.5_unshrunk</th>\n",
       "      <th>3_shrunk</th>\n",
       "      <th>3_unshrunk</th>\n",
       "      <th>4_shrunk</th>\n",
       "      <th>4_unshrunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>2.078958</td>\n",
       "      <td>2.096641</td>\n",
       "      <td>2.610209</td>\n",
       "      <td>2.639680</td>\n",
       "      <td>2.344583</td>\n",
       "      <td>2.368160</td>\n",
       "      <td>3.141460</td>\n",
       "      <td>3.182720</td>\n",
       "      <td>2.875835</td>\n",
       "      <td>2.911200</td>\n",
       "      <td>3.407086</td>\n",
       "      <td>3.454240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>3.040413</td>\n",
       "      <td>3.117582</td>\n",
       "      <td>4.544566</td>\n",
       "      <td>4.673180</td>\n",
       "      <td>3.792489</td>\n",
       "      <td>3.895381</td>\n",
       "      <td>6.048718</td>\n",
       "      <td>6.228778</td>\n",
       "      <td>5.296642</td>\n",
       "      <td>5.450979</td>\n",
       "      <td>6.800794</td>\n",
       "      <td>7.006577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>2.290933</td>\n",
       "      <td>2.369524</td>\n",
       "      <td>3.200393</td>\n",
       "      <td>3.331376</td>\n",
       "      <td>2.745663</td>\n",
       "      <td>2.850450</td>\n",
       "      <td>4.109852</td>\n",
       "      <td>4.293229</td>\n",
       "      <td>3.655122</td>\n",
       "      <td>3.812303</td>\n",
       "      <td>4.564582</td>\n",
       "      <td>4.774156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>3.223711</td>\n",
       "      <td>3.336902</td>\n",
       "      <td>4.959738</td>\n",
       "      <td>5.148390</td>\n",
       "      <td>4.091725</td>\n",
       "      <td>4.242646</td>\n",
       "      <td>6.695766</td>\n",
       "      <td>6.959878</td>\n",
       "      <td>5.827752</td>\n",
       "      <td>6.054134</td>\n",
       "      <td>7.563779</td>\n",
       "      <td>7.865622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>2.982469</td>\n",
       "      <td>3.184680</td>\n",
       "      <td>5.072223</td>\n",
       "      <td>5.409241</td>\n",
       "      <td>4.027346</td>\n",
       "      <td>4.296960</td>\n",
       "      <td>7.161977</td>\n",
       "      <td>7.633801</td>\n",
       "      <td>6.117100</td>\n",
       "      <td>6.521521</td>\n",
       "      <td>8.206854</td>\n",
       "      <td>8.746082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>3.116358</td>\n",
       "      <td>3.262773</td>\n",
       "      <td>5.107134</td>\n",
       "      <td>5.351160</td>\n",
       "      <td>4.111746</td>\n",
       "      <td>4.306967</td>\n",
       "      <td>7.097910</td>\n",
       "      <td>7.439546</td>\n",
       "      <td>6.102522</td>\n",
       "      <td>6.395353</td>\n",
       "      <td>8.093298</td>\n",
       "      <td>8.483739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>2.195580</td>\n",
       "      <td>2.423192</td>\n",
       "      <td>4.589023</td>\n",
       "      <td>4.968376</td>\n",
       "      <td>3.392301</td>\n",
       "      <td>3.695784</td>\n",
       "      <td>6.982466</td>\n",
       "      <td>7.513560</td>\n",
       "      <td>5.785744</td>\n",
       "      <td>6.240968</td>\n",
       "      <td>8.179187</td>\n",
       "      <td>8.786152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1.5_shrunk  1.5_unshrunk  2.5_shrunk  2.5_unshrunk  2_shrunk  \\\n",
       "US_RE         2.078958      2.096641    2.610209      2.639680  2.344583   \n",
       "US_PE         3.040413      3.117582    4.544566      4.673180  3.792489   \n",
       "US_HY         2.290933      2.369524    3.200393      3.331376  2.745663   \n",
       "SP500         3.223711      3.336902    4.959738      5.148390  4.091725   \n",
       "Rusell2000    2.982469      3.184680    5.072223      5.409241  4.027346   \n",
       "EAFE          3.116358      3.262773    5.107134      5.351160  4.111746   \n",
       "EM            2.195580      2.423192    4.589023      4.968376  3.392301   \n",
       "\n",
       "            2_unshrunk  3.5_shrunk  3.5_unshrunk  3_shrunk  3_unshrunk  \\\n",
       "US_RE         2.368160    3.141460      3.182720  2.875835    2.911200   \n",
       "US_PE         3.895381    6.048718      6.228778  5.296642    5.450979   \n",
       "US_HY         2.850450    4.109852      4.293229  3.655122    3.812303   \n",
       "SP500         4.242646    6.695766      6.959878  5.827752    6.054134   \n",
       "Rusell2000    4.296960    7.161977      7.633801  6.117100    6.521521   \n",
       "EAFE          4.306967    7.097910      7.439546  6.102522    6.395353   \n",
       "EM            3.695784    6.982466      7.513560  5.785744    6.240968   \n",
       "\n",
       "            4_shrunk  4_unshrunk  \n",
       "US_RE       3.407086    3.454240  \n",
       "US_PE       6.800794    7.006577  \n",
       "US_HY       4.564582    4.774156  \n",
       "SP500       7.563779    7.865622  \n",
       "Rusell2000  8.206854    8.746082  \n",
       "EAFE        8.093298    8.483739  \n",
       "EM          8.179187    8.786152  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implied_ExpRet['weight_peer']*400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mean variance optimization, constuct efficient fronter \n",
    "\n",
    "CMA_ExpRet_geo= np.array( [700, 880, 477, 721, 806, 707, 803]) /10000 /4 #quarterly expected exponential ret \n",
    "LW_cov.index\n",
    "CMA_ExpRet_arith= CMA_ExpRet_geo+ .5* np.diag(LW_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 6\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 6\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 26\n",
      "            Function evaluations: 182\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 26\n",
      "            Function evaluations: 182\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 53\n",
      "            Function evaluations: 441\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 53\n",
      "            Function evaluations: 441\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 18\n",
      "            Function evaluations: 134\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 18\n",
      "            Function evaluations: 134\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 24\n",
      "            Function evaluations: 166\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 24\n",
      "            Function evaluations: 166\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 54\n",
      "            Function evaluations: 530\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 54\n",
      "            Function evaluations: 530\n",
      "            Gradient evaluations: 50\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 74\n",
      "            Function evaluations: 664\n",
      "            Gradient evaluations: 70\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 74\n",
      "            Function evaluations: 664\n",
      "            Gradient evaluations: 70\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 27\n",
      "            Function evaluations: 191\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 27\n",
      "            Function evaluations: 191\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 43\n",
      "            Function evaluations: 394\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 43\n",
      "            Function evaluations: 394\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 18\n",
      "            Function evaluations: 125\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 18\n",
      "            Function evaluations: 125\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 38\n",
      "            Function evaluations: 316\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 38\n",
      "            Function evaluations: 316\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 22\n",
      "            Function evaluations: 178\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 22\n",
      "            Function evaluations: 178\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 43\n",
      "            Function evaluations: 403\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215813962246\n",
      "            Iterations: 43\n",
      "            Function evaluations: 403\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00211489795434\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00211489795434\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00202398812803\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00202398812803\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00193688433199\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00193688433199\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00185358656622\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00185358656622\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00177409483073\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00177409483073\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00169840912551\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00169840912551\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00162652945057\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00162652945057\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0015584558059\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0015584558059\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00149418819151\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00149418819151\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00144021460436\n",
      "            Iterations: 4\n",
      "            Function evaluations: 4\n",
      "            Gradient evaluations: 4\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00143372660739\n",
      "            Iterations: 5\n",
      "            Function evaluations: 5\n",
      "            Gradient evaluations: 5\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00141798064189\n",
      "            Iterations: 9\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00137707105354\n",
      "            Iterations: 6\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00139688088257\n",
      "            Iterations: 7\n",
      "            Function evaluations: 7\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00132422152997\n",
      "            Iterations: 6\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00137663476492\n",
      "            Iterations: 7\n",
      "            Function evaluations: 7\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127517803668\n",
      "            Iterations: 6\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00135724228893\n",
      "            Iterations: 8\n",
      "            Function evaluations: 8\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122994057365\n",
      "            Iterations: 6\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0013387034546\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118850914091\n",
      "            Iterations: 8\n",
      "            Function evaluations: 8\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00132101826192\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115088373843\n",
      "            Iterations: 10\n",
      "            Function evaluations: 10\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00130418671091\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00111706436623\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00128820880156\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00108705102431\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127308453387\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00106084371266\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00125881390784\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00103844243128\n",
      "            Iterations: 13\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00124539692347\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00101984718018\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00123283358076\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00100505795935\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122112387971\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000993705454421\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00121026782032\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000983574776685\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00120026540259\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000974296944229\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00119111662652\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000965871957053\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118282149211\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000958299815158\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117537999937\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000951580518544\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116879214828\n",
      "            Iterations: 13\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00094571406721\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116305793885\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000940700461157\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115817737108\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000936539700385\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115415044498\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000933231784892\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115097716053\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000930776714681\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00114865751774\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00092917448975\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00114719151662\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000928425110099\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00114657915715\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000928528575729\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00114682043935\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00092948488664\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0011479153632\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000931294042831\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00114986392872\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000933956044302\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115266613589\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000937470891055\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115632198473\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000941838583087\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116083147523\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000947059120401\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116619460738\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000953132502994\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0011724113812\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000960058730869\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117962121764\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000967837804024\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0011884909081\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000976469722459\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00119910740508\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000985954486175\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00121147070857\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000996292095171\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122549141004\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00100748254945\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00124107999543\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00101952584901\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00125823614133\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00103242199384\n",
      "            Iterations: 20\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127662159068\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00104617098396\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00129596109141\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00106070203353\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0013162546435\n",
      "            Iterations: 31\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00107591017355\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00133750224697\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00109179443026\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00135970390203\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00110835480365\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00138285960801\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00112559129374\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00140690407117\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00114372851241\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00143177097721\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116386950454\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00145746023592\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118616259688\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00148397193645\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00121060778941\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0015113060491\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00123720508213\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00153946257385\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00126595447506\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00156844151071\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00129685596818\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00159824285967\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00132990956149\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00162886662075\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.001365115255\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00166031279393\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00140247304871\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00169258137922\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00144198294261\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0017256723767\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00148364493671\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00175958578614\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00152745903101\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00179434386436\n",
      "            Iterations: 28\n",
      "            Function evaluations: 28\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0015734252255\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00183069943445\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00162154352019\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00186902087418\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00167181391508\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00190930818358\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00172423641016\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00195156136262\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00177881100544\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0019957804113\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00183553770092\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00204196601103\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00189441649659\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00209605136836\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00195544739246\n",
      "            Iterations: 19\n",
      "            Function evaluations: 19\n",
      "            Gradient evaluations: 19\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00216478256809\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00201863038852\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00224308820658\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00208396548478\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00232412719082\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215145268124\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00240786298033\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00222109197789\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0024942955751\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00229288337474\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00258342497515\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00236682687179\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00267525118046\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00244292246903\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00276977419104\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00252117016647\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00286699400689\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00260156996411\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def obj_func(w, sigma):\n",
    "    return (np.dot(  np.dot( w, sigma), w)* .5)\n",
    "\n",
    "def obj_func_derivative( w, sigma): \n",
    "    return (np.dot( w, sigma))\n",
    "\n",
    "\n",
    "cons_eq0= {'type': 'eq',\n",
    "          'fun': lambda w: np.array( [np.sum(w)-1] ) ,\n",
    "          'jac': lambda w: np.array( [1]*N)}\n",
    "# cons_ineq0= {'type': 'ineq', \n",
    "#              'fun': lambda w: -w[0]-w[1]+ 0.5, \n",
    "#              'jac': lambda w: np.array([-1]*2+ [0]*10)}\n",
    "# cons_ineq1= {'type': 'ineq',\n",
    "#              'fun': lambda w: -w[2]-w[3]-w[4]-w[5]-w[6]+ 0.5,\n",
    "#              'jac': lambda w: np.array([0]*2+ [-1]*5+ [0]*5)}\n",
    "# cons_ineq2= {'type': 'ineq', \n",
    "#              'fun': lambda w: -w[7]-w[8]-w[9]-w[10]+ 0.5, \n",
    "#              'jac': lambda w: np.array([0]*7+ [-1]*4+ [0])}\n",
    "# cons_ineq3= {'type': 'ineq', \n",
    "#             'fun': lambda w: -w[11]+ 0.5,\n",
    "#             'jac': lambda w: np.array( [0]*11+ [-1])}\n",
    "\n",
    "\n",
    "\n",
    "fronter1_w= {}\n",
    "fronter1_vol= {}\n",
    "fronter2_w= {}\n",
    "fronter2_vol= {}\n",
    "\n",
    "for target_ret in np.linspace(0.05, 0.1, 100 ): \n",
    "    cons_ineq4= {'type': 'eq', \n",
    "                'fun': lambda w: -np.dot(w, CMA_ExpRet_arith*4)+ target_ret,\n",
    "                'jac': lambda w: -CMA_ExpRet_arith*4}\n",
    "\n",
    "    cons= (cons_eq0\n",
    "    #        cons_ineq0,\n",
    "    #        cons_ineq1,\n",
    "    #        cons_ineq2,\n",
    "    #        cons_ineq3\n",
    "          , cons_ineq4\n",
    "          )\n",
    "\n",
    "    MV_opt_2= minimize( obj_func, \n",
    "                    x0= weight_eq, \n",
    "                    args= LW_cov, \n",
    "                    jac= obj_func_derivative ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0,0.25]]+[[0, 0.4]]+[[0,None]]* (N-2),\n",
    "                    tol= 1e-12)  # long only+ concentration constrain\n",
    "\n",
    "    MV_opt_1= minimize( obj_func, \n",
    "                    x0= weight_eq, \n",
    "                    args= LW_cov, \n",
    "                    jac= obj_func_derivative ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0, None]]* N,\n",
    "                    tol= 1e-12)  # long only constrain\n",
    "    \n",
    "    fronter1_w[target_ret]= MV_opt_1.x\n",
    "    fronter1_vol[target_ret]= np.sqrt(MV_opt_1.fun*2) \n",
    "    \n",
    "    fronter2_w[target_ret]= MV_opt_2.x\n",
    "    fronter2_vol[target_ret]= np.sqrt(MV_opt_2.fun*2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.084343434343434345, 0.047081945661880828)\n",
      "[  5.76958650e-01   3.92525584e-01   1.58869429e-02   0.00000000e+00\n",
      "   1.51788304e-18   5.28548559e-19   1.46288226e-02]\n",
      "(0.082828282828282834, 0.050910924002886389)\n",
      "[ 0.25        0.4         0.18071297  0.13419572  0.          0.0211764\n",
      "  0.0139149 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAFyCAYAAABRKmj1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREckJBh4iIiOSEgg4RERHJif8PvyvqFfbXg3MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xcfddcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fronter_1= list( fronter1_vol.items())\n",
    "fronter_1.sort( key=lambda x: x[1])\n",
    "minvar_portf1= fronter_1[0]\n",
    "minvar_portf1_w= fronter1_w[minvar_portf1[0]]\n",
    "fronter_1.sort( key= lambda x: (x[0]- rf)/ x[1], reverse=True)\n",
    "efficient_portf1= fronter_1[0]\n",
    "efficient_portf1_w= fronter1_w[efficient_portf1[0]]\n",
    "\n",
    "fronter_2= list(fronter2_vol.items())\n",
    "fronter_2.sort(key= lambda x: x[1])\n",
    "minvar_portf2= fronter_2[0]\n",
    "minvar_portf2_w= fronter2_w[minvar_portf2[0]]\n",
    "fronter_2.sort(key= lambda x: (x[0]-rf)/x[1], reverse= True)\n",
    "efficient_portf2= fronter_2[0]\n",
    "efficient_portf2_w= fronter2_w[efficient_portf2[0]]\n",
    "\n",
    "[tmp_ret, tmp_vol]= zip( * list( fronter1_vol.items())) \n",
    "fig= plt.figure( )\n",
    "plt.scatter(x= tmp_vol, y= tmp_ret, marker=\".\", c= 'pink' , label= 'long only')\n",
    "plt.scatter(x= minvar_portf1[1], y= minvar_portf1[0], marker= 'o', c='purple', s= 100 )\n",
    "plt.scatter(x= efficient_portf1[1], y = efficient_portf1[0], marker= '*', c='m', s=200)\n",
    "plt.plot( [0.04, efficient_portf1[1]], [ (efficient_portf1[0]-rf)/efficient_portf1[1]* 0.04+rf, efficient_portf1[0]], 'k-')\n",
    "\n",
    "[tmp_ret, tmp_vol]= zip(* list(fronter2_vol.items()))\n",
    "plt.scatter( x= tmp_vol, y= tmp_ret, marker= '.', c='blue', label= 'long only+ concentration constrain')\n",
    "plt.scatter( x= minvar_portf2[1], y=minvar_portf2[0], marker= 'o', c= 'y', s=100)\n",
    "plt.scatter(x= efficient_portf2[1], y = efficient_portf2[0], marker= '*', c='r', s=200)\n",
    "plt.plot( [0.04, efficient_portf2[1]], [ (efficient_portf2[0]-rf)/efficient_portf2[1]* 0.04+rf, efficient_portf2[0]], 'k-')\n",
    "plt.legend()\n",
    "plt.title('Efficient Fronter')\n",
    "\n",
    "print(efficient_portf1)\n",
    "print(efficient_portf1_w)\n",
    "print(efficient_portf2)\n",
    "print(efficient_portf2_w)\n",
    "\n",
    "weight_longonly= efficient_portf1_w\n",
    "weight_longonly_conc= efficient_portf2_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_eq</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_peer</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>2.970297e-02</td>\n",
       "      <td>2.079208e-01</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_erc</th>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.142979</td>\n",
       "      <td>0.177851</td>\n",
       "      <td>0.118734</td>\n",
       "      <td>9.272408e-02</td>\n",
       "      <td>1.058735e-01</td>\n",
       "      <td>0.079423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_weight_longonly</th>\n",
       "      <td>0.576959</td>\n",
       "      <td>0.392526</td>\n",
       "      <td>0.015887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.517883e-18</td>\n",
       "      <td>5.285486e-19</td>\n",
       "      <td>0.014629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_weight_longonly_conc</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.180713</td>\n",
       "      <td>0.134196</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.117640e-02</td>\n",
       "      <td>0.013915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             US_RE     US_PE     US_HY     SP500  \\\n",
       "weight_eq                 0.142857  0.142857  0.142857  0.142857   \n",
       "weight_peer               0.138614  0.287129  0.049505  0.237624   \n",
       "weight_erc                0.282415  0.142979  0.177851  0.118734   \n",
       "CMA_weight_longonly       0.576959  0.392526  0.015887  0.000000   \n",
       "CMA_weight_longonly_conc  0.250000  0.400000  0.180713  0.134196   \n",
       "\n",
       "                            Rusell2000          EAFE        EM  \n",
       "weight_eq                 1.428571e-01  1.428571e-01  0.142857  \n",
       "weight_peer               2.970297e-02  2.079208e-01  0.049505  \n",
       "weight_erc                9.272408e-02  1.058735e-01  0.079423  \n",
       "CMA_weight_longonly       1.517883e-18  5.285486e-19  0.014629  \n",
       "CMA_weight_longonly_conc  0.000000e+00  2.117640e-02  0.013915  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portf_weight_2= pd.DataFrame([efficient_portf1_w, efficient_portf2_w], \n",
    "                             index=['CMA_weight_longonly', 'CMA_weight_longonly_conc'], columns=LW_cov.columns)\n",
    "pd.concat([portf_weight_1, portf_weight_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0196717373343\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0186336386178\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0176311709875\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0167000336455\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0158337032563\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n"
     ]
    }
   ],
   "source": [
    "## risk adj return optimal long only portfolio with CMA expected ret\n",
    "\n",
    "\n",
    "def obj_func_CMA(w, ARGS):  # ARGS= [sigma, ExpRet, gamma]\n",
    "    return (np.dot(  np.dot( w, ARGS[0]), w)* .5* ARGS[2]- np.dot( ARGS[1], w))\n",
    "\n",
    "def obj_func_derivative_CMA( w, ARGS): \n",
    "    return (np.dot( w, ARGS[0])* ARGS[2]- ARGS[1])\n",
    "\n",
    "\n",
    "cons_eq0= {'type': 'eq',\n",
    "          'fun': lambda w: np.array( [np.sum(w)-1] ) ,\n",
    "          'jac': lambda w: np.array( [1]*N)}\n",
    "# cons_ineq0= {'type': 'ineq', \n",
    "#              'fun': lambda w: -w[0]-w[1]+ 0.5, \n",
    "#              'jac': lambda w: np.array([-1]*2+ [0]*10)}\n",
    "# cons_ineq1= {'type': 'ineq',\n",
    "#              'fun': lambda w: -w[2]-w[3]-w[4]-w[5]-w[6]+ 0.5,\n",
    "#              'jac': lambda w: np.array([0]*2+ [-1]*5+ [0]*5)}\n",
    "# cons_ineq2= {'type': 'ineq', \n",
    "#              'fun': lambda w: -w[7]-w[8]-w[9]-w[10]+ 0.5, \n",
    "#              'jac': lambda w: np.array([0]*7+ [-1]*4+ [0])}\n",
    "# cons_ineq3= {'type': 'ineq', \n",
    "#             'fun': lambda w: -w[11]+ 0.5,\n",
    "#             'jac': lambda w: np.array( [0]*11+ [-1])}\n",
    "\n",
    "\n",
    "CMA_riskAdj_opt={}\n",
    "\n",
    "for g in [2,2.5,3,3.5,4]: \n",
    "\n",
    "    cons= (cons_eq0\n",
    "    #        cons_ineq0,\n",
    "    #        cons_ineq1,\n",
    "    #        cons_ineq2,\n",
    "    #        cons_ineq3\n",
    "          )\n",
    "\n",
    "    MV_opt= minimize( obj_func_CMA, \n",
    "                    x0= weight_eq, \n",
    "                    args= [LW_cov, CMA_ExpRet_arith, g], \n",
    "                    jac= obj_func_derivative_CMA ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0, 0.25]]+[[0, .4]]+[[0,None]]* (N-2),\n",
    "                    tol= 1e-12)\n",
    "    \n",
    "    CMA_riskAdj_opt[g]= MV_opt.x\n",
    "    \n",
    "CMA_riskAdj_portf_w= pd.DataFrame( CMA_riskAdj_opt, index=LW_cov.columns).T\n",
    "CMA_riskAdj_portf_w.index= ['weight_CMA_MVO_gamma_'+str(x) for x in CMA_riskAdj_portf_w.index]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_CMA_MVO_gamma_2.0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7.334987e-17</td>\n",
       "      <td>7.179246e-17</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>4.093808e-17</td>\n",
       "      <td>0.207309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_CMA_MVO_gamma_2.5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.894287e-17</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.178579</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.171421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_CMA_MVO_gamma_3.0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.393563e-02</td>\n",
       "      <td>0.147340</td>\n",
       "      <td>1.145913e-02</td>\n",
       "      <td>0.137265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_CMA_MVO_gamma_3.5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.185327e-01</td>\n",
       "      <td>0.099782</td>\n",
       "      <td>2.314467e-02</td>\n",
       "      <td>0.108540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_CMA_MVO_gamma_4.0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.077001e-02</td>\n",
       "      <td>1.363963e-01</td>\n",
       "      <td>0.063692</td>\n",
       "      <td>2.457743e-02</td>\n",
       "      <td>0.084564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          US_RE  US_PE         US_HY         SP500  \\\n",
       "weight_CMA_MVO_gamma_2.0   0.25    0.4  7.334987e-17  7.179246e-17   \n",
       "weight_CMA_MVO_gamma_2.5   0.25    0.4  8.894287e-17  0.000000e+00   \n",
       "weight_CMA_MVO_gamma_3.0   0.25    0.4  0.000000e+00  5.393563e-02   \n",
       "weight_CMA_MVO_gamma_3.5   0.25    0.4  0.000000e+00  1.185327e-01   \n",
       "weight_CMA_MVO_gamma_4.0   0.25    0.4  4.077001e-02  1.363963e-01   \n",
       "\n",
       "                          Rusell2000          EAFE        EM  \n",
       "weight_CMA_MVO_gamma_2.0    0.142691  4.093808e-17  0.207309  \n",
       "weight_CMA_MVO_gamma_2.5    0.178579  0.000000e+00  0.171421  \n",
       "weight_CMA_MVO_gamma_3.0    0.147340  1.145913e-02  0.137265  \n",
       "weight_CMA_MVO_gamma_3.5    0.099782  2.314467e-02  0.108540  \n",
       "weight_CMA_MVO_gamma_4.0    0.063692  2.457743e-02  0.084564  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CMA_riskAdj_portf_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Introducing active management\n",
    "# Apply active management to Equity (SP500, Rusell2000, EAFE and EM), \n",
    "# assuming IR of 1/3 and active alpha 1% and hence tracking error 3%, both annualized \n",
    "\n",
    "\n",
    "LW_vol= np.sqrt(np.diag(LW_cov))\n",
    "LW_corr= pd.DataFrame(np.dot(np.dot(np.diag(1/LW_vol), LW_cov), np.diag(1/LW_vol)), columns= LW_cov.columns, index=LW_cov.index)\n",
    "LW_cov_active= pd.DataFrame(LW_cov+ np.diag( np.array([0, 0, 0.0009/4, 0.0009/4, 0.0009/4, 0.0009/4, 0.0009/4])),\n",
    "                           index= LW_cov.index,\n",
    "                           columns= LW_cov.columns)\n",
    "LW_vol_active= np.sqrt(np.diag(LW_cov_active))\n",
    "\n",
    "CMA_ExpRet_active_geo= CMA_ExpRet_geo+ np.array([0,0,0.0075/4, 0.01/4, 0.01/4, 0.01/4, 0.01/4])\n",
    "CMA_ExpRet_active_arith= CMA_ExpRet_arith+ np.array( [0,0,0.0075/4+ .5* 0.0009/4, 0.01/4+ .5* 0.0009/4, 0.01/4+ .5* 0.0009/4, 0.01/4+ .5* 0.0009/4, 0.01/4+ .5* 0.0009/4 ])\n",
    "\n",
    "LW_corr_active= pd.DataFrame(np.dot(np.dot(np.diag(1/LW_vol_active), LW_cov_active), np.diag(1/LW_vol_active)), \n",
    "                            index= LW_cov_active.index,\n",
    "                            columns= LW_cov_active.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0020345040048\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0020345040048\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00192702470341\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00192702470341\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00182540926847\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00182540926847\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00172965769997\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00172965769997\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00163976999793\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00163976999793\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00155574616234\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00155574616234\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00149939636096\n",
      "            Iterations: 5\n",
      "            Function evaluations: 5\n",
      "            Gradient evaluations: 5\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0014775861932\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00147874912736\n",
      "            Iterations: 6\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00140529009052\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00145876513432\n",
      "            Iterations: 6\n",
      "            Function evaluations: 6\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00133885785428\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00143944438182\n",
      "            Iterations: 7\n",
      "            Function evaluations: 7\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127828948449\n",
      "            Iterations: 2\n",
      "            Function evaluations: 2\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00142078686987\n",
      "            Iterations: 7\n",
      "            Function evaluations: 7\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122358498115\n",
      "            Iterations: 4\n",
      "            Function evaluations: 4\n",
      "            Gradient evaluations: 4\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00140279259847\n",
      "            Iterations: 7\n",
      "            Function evaluations: 7\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117474434427\n",
      "            Iterations: 7\n",
      "            Function evaluations: 7\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00138546156762\n",
      "            Iterations: 9\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00113176757383\n",
      "            Iterations: 8\n",
      "            Function evaluations: 8\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00136879377732\n",
      "            Iterations: 10\n",
      "            Function evaluations: 10\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00109465466985\n",
      "            Iterations: 9\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00135278922757\n",
      "            Iterations: 10\n",
      "            Function evaluations: 10\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00106340563231\n",
      "            Iterations: 9\n",
      "            Function evaluations: 9\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00133744791836\n",
      "            Iterations: 10\n",
      "            Function evaluations: 10\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00103802046123\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0013227698497\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00101849915659\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0013087550216\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00100484171841\n",
      "            Iterations: 13\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00129540343404\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000995617527367\n",
      "            Iterations: 13\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00128271508703\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000987203310446\n",
      "            Iterations: 13\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127068998057\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000979421646194\n",
      "            Iterations: 13\n",
      "            Function evaluations: 13\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00125932811465\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000972272534612\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00124862948929\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0009657559757\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00123859410447\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000959871969456\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122922196021\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000954620515882\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122051305649\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000950001614978\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00121246739332\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000946015266743\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0012050849707\n",
      "            Iterations: 11\n",
      "            Function evaluations: 11\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000942661471177\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00119836578863\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00093994022828\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0011923098471\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000937851538053\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118691714613\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000936395400495\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118217542302\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000935571815607\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117803804408\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000935380783388\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117450047273\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000935822303838\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117156270896\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000936896376957\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116922475279\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000938581575345\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116748660419\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000940833170767\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116634826319\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000943649764069\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116580972976\n",
      "            Iterations: 12\n",
      "            Function evaluations: 12\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000947031355252\n",
      "            Iterations: 16\n",
      "            Function evaluations: 16\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116587100393\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000950977944315\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116653208568\n",
      "            Iterations: 14\n",
      "            Function evaluations: 14\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000955489531259\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116779297501\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000960566116083\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116965367194\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000966207698789\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117211417644\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000972413133788\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117517448854\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000979165192282\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117883460822\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000986458090358\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118309453548\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.000994291827006\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00118795427033\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00100266640335\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00119341381277\n",
      "            Iterations: 15\n",
      "            Function evaluations: 15\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0010115818187\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00119947316279\n",
      "            Iterations: 17\n",
      "            Function evaluations: 17\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00102098131837\n",
      "            Iterations: 35\n",
      "            Function evaluations: 36\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0012061323204\n",
      "            Iterations: 18\n",
      "            Function evaluations: 18\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00103077749334\n",
      "            Iterations: 36\n",
      "            Function evaluations: 36\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00121339127606\n",
      "            Iterations: 20\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0010409693728\n",
      "            Iterations: 35\n",
      "            Function evaluations: 35\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122123611301\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00105155695675\n",
      "            Iterations: 34\n",
      "            Function evaluations: 35\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122972793608\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00106254024527\n",
      "            Iterations: 37\n",
      "            Function evaluations: 38\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00123891406128\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00107391923833\n",
      "            Iterations: 37\n",
      "            Function evaluations: 38\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00124879448863\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00108569393592\n",
      "            Iterations: 39\n",
      "            Function evaluations: 39\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00125936921812\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00109786433806\n",
      "            Iterations: 39\n",
      "            Function evaluations: 39\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127063824974\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00111043044478\n",
      "            Iterations: 38\n",
      "            Function evaluations: 38\n",
      "            Gradient evaluations: 38\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00128259662283\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00112339225605\n",
      "            Iterations: 36\n",
      "            Function evaluations: 36\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00129507557599\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00113674977157\n",
      "            Iterations: 29\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 29\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00130799235398\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00115050299179\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00132134695679\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00116465191656\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00133513938492\n",
      "            Iterations: 31\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00117920405667\n",
      "            Iterations: 35\n",
      "            Function evaluations: 35\n",
      "            Gradient evaluations: 35\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00134936963711\n",
      "            Iterations: 31\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00119433545498\n",
      "            Iterations: 39\n",
      "            Function evaluations: 39\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00136403771415\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00121011802708\n",
      "            Iterations: 31\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00137914361625\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00122655149068\n",
      "            Iterations: 39\n",
      "            Function evaluations: 39\n",
      "            Gradient evaluations: 39\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00139468255175\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00124363612796\n",
      "            Iterations: 37\n",
      "            Function evaluations: 37\n",
      "            Gradient evaluations: 37\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00141061312215\n",
      "            Iterations: 28\n",
      "            Function evaluations: 28\n",
      "            Gradient evaluations: 28\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00126137184484\n",
      "            Iterations: 36\n",
      "            Function evaluations: 37\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00142692577675\n",
      "            Iterations: 31\n",
      "            Function evaluations: 31\n",
      "            Gradient evaluations: 31\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00127975864174\n",
      "            Iterations: 36\n",
      "            Function evaluations: 36\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00144362051635\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00129879651745\n",
      "            Iterations: 34\n",
      "            Function evaluations: 35\n",
      "            Gradient evaluations: 34\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00146069734079\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00131848547375\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00147815624998\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00133882550851\n",
      "            Iterations: 27\n",
      "            Function evaluations: 27\n",
      "            Gradient evaluations: 27\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00149599724398\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00135981662346\n",
      "            Iterations: 32\n",
      "            Function evaluations: 32\n",
      "            Gradient evaluations: 32\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00151422032276\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00138145881803\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00153282548633\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00140375584449\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00155181273469\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00142676256837\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00157118206783\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00145050768271\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00159093348575\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00147499118752\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00161106698846\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0015002130828\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00163158257596\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00152617336855\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00165248024824\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00155287204476\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0016737600053\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00158030911144\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00169542184716\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00160848456858\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00171746577379\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00163739841619\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00173989178521\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00166705065426\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00176272237777\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00169744128281\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00178641790708\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00172857030182\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0018111603211\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00176043771129\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00183694961981\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00179304351123\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00186378589373\n",
      "            Iterations: 20\n",
      "            Function evaluations: 20\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00182638770164\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00189166887132\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00186047028251\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00192059882412\n",
      "            Iterations: 25\n",
      "            Function evaluations: 25\n",
      "            Gradient evaluations: 25\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00189529125385\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00195057566162\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00193085061566\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00198159938382\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00196714836793\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00201366999072\n",
      "            Iterations: 30\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 30\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00200418451067\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00204678748242\n",
      "            Iterations: 26\n",
      "            Function evaluations: 26\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00204195904388\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0020828285226\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00208047196755\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00212235883996\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00211972328169\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00216264315315\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00215971298629\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00220368146215\n",
      "            Iterations: 23\n",
      "            Function evaluations: 23\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.00220044108136\n",
      "            Iterations: 24\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 24\n"
     ]
    }
   ],
   "source": [
    "## Fronter Construction with active management\n",
    "\n",
    "\n",
    "\n",
    "fronter1_active_w= {}\n",
    "fronter1_active_vol= {}\n",
    "fronter2_active_w= {}\n",
    "fronter2_active_vol= {}\n",
    "\n",
    "for target_ret in np.linspace(0.065, 0.1, 100 ): \n",
    "    cons_ineq4_active= {'type': 'eq', \n",
    "                'fun': lambda w: -np.dot(w, CMA_ExpRet_active_arith*4)+ target_ret,\n",
    "                'jac': lambda w: -CMA_ExpRet_active_arith*4}\n",
    "\n",
    "    cons= (cons_eq0\n",
    "    #        cons_ineq0,\n",
    "    #        cons_ineq1,\n",
    "    #        cons_ineq2,\n",
    "    #        cons_ineq3\n",
    "          , cons_ineq4_active\n",
    "          )\n",
    "\n",
    "    MV_active_opt_2= minimize( obj_func, \n",
    "                    x0= weight_eq, \n",
    "                    args= LW_cov_active, \n",
    "                    jac= obj_func_derivative ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0,0.25]]+[[0, 0.4]]+[[0,None]]* (N-2),\n",
    "                    tol= 1e-12)  # long only+ concentration constrain\n",
    "\n",
    "    MV_active_opt_1= minimize( obj_func, \n",
    "                    x0= weight_eq, \n",
    "                    args= LW_cov_active, \n",
    "                    jac= obj_func_derivative ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0, None]]* N,\n",
    "                    tol= 1e-12)  # long only constrain\n",
    "    \n",
    "    fronter1_active_w[target_ret]= MV_active_opt_1.x\n",
    "    fronter1_active_vol[target_ret]= np.sqrt(MV_active_opt_1.fun*2) \n",
    "    \n",
    "    fronter2_active_w[target_ret]= MV_active_opt_2.x\n",
    "    fronter2_active_vol[target_ret]= np.sqrt(MV_active_opt_2.fun*2)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.084343434343434345, 0.047081945661880828)\n",
      "[  5.76958650e-01   3.92525584e-01   1.58869429e-02   0.00000000e+00\n",
      "   1.51788304e-18   5.28548559e-19   1.46288226e-02]\n",
      "(0.082828282828282834, 0.050910924002886389)\n",
      "[ 0.25        0.4         0.18071297  0.13419572  0.          0.0211764\n",
      "  0.0139149 ]\n",
      "(0.084090909090909091, 0.04634477831055471)\n",
      "[  5.39163618e-01   3.12269699e-01   5.54793646e-02   5.79459741e-02\n",
      "   1.75674633e-18   1.39161193e-02   2.12252243e-02]\n",
      "(0.086565656565656568, 0.051407138741341132)\n",
      "[ 0.25        0.4         0.1641722   0.14092414  0.          0.03092334\n",
      "  0.01398031]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABloAAANDCAYAAADFLMKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc5405f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fronter_1= list( fronter1_vol.items())\n",
    "fronter_1.sort( key=lambda x: x[1])\n",
    "minvar_portf1= fronter_1[0]\n",
    "minvar_portf1_w= fronter1_w[minvar_portf1[0]]\n",
    "fronter_1.sort( key= lambda x: (x[0]- rf)/ x[1], reverse=True)\n",
    "efficient_portf1= fronter_1[0]\n",
    "efficient_portf1_w= fronter1_w[efficient_portf1[0]]\n",
    "\n",
    "fronter_2= list(fronter2_vol.items())\n",
    "fronter_2.sort(key= lambda x: x[1])\n",
    "minvar_portf2= fronter_2[0]\n",
    "minvar_portf2_w= fronter2_w[minvar_portf2[0]]\n",
    "fronter_2.sort(key= lambda x: (x[0]-rf)/x[1], reverse= True)\n",
    "efficient_portf2= fronter_2[0]\n",
    "efficient_portf2_w= fronter2_w[efficient_portf2[0]]\n",
    "\n",
    "fronter_active_1= list( fronter1_active_vol.items())\n",
    "fronter_active_1.sort(key= lambda x: x[1])\n",
    "minvar_active_portf1= fronter_active_1[0]\n",
    "minvar_active_portf1_w= fronter1_active_w[minvar_active_portf1[0]]\n",
    "fronter_active_1.sort(key= lambda x: (x[0]-rf)/x[1], reverse =True)\n",
    "efficient_portf1_active= fronter_active_1[0]\n",
    "efficient_portf1_active_w= fronter1_active_w[efficient_portf1_active[0]]\n",
    "\n",
    "\n",
    "fronter_active_2= list( fronter2_active_vol.items())\n",
    "fronter_active_2.sort( key= lambda x: x[1])\n",
    "minvar_active_portf2= fronter_active_2[0]\n",
    "minvar_active_portf2_w= fronter2_active_w[ minvar_active_portf2[0]]\n",
    "fronter_active_2.sort(key= lambda x: (x[0]-rf)/x[1], reverse= True)\n",
    "efficient_portf2_active= fronter_active_2[0]\n",
    "efficient_portf2_active_w= fronter2_active_w[efficient_portf2_active[0]]\n",
    "\n",
    "\n",
    "fig= plt.figure(figsize= (20,10))\n",
    "\n",
    "[tmp_ret, tmp_vol]= zip( * list( fronter1_vol.items())) \n",
    "plt.scatter(x= tmp_vol, y= tmp_ret, marker=\".\", c= 'pink' , label= 'CMA:long only')\n",
    "plt.scatter(x= minvar_portf1[1], y= minvar_portf1[0], marker= 'o', c='pink', s= 100 )\n",
    "plt.scatter(x= efficient_portf1[1], y = efficient_portf1[0], marker= '*', c='pink', s=200)\n",
    "# plt.plot( [0.04, efficient_portf1[1]], [ (efficient_portf1[0]-rf)/efficient_portf1[1]* 0.04+rf, efficient_portf1[0]], \n",
    "#          linestyle='-', c='pink')\n",
    "\n",
    "[tmp_ret, tmp_vol]= zip(* list(fronter2_vol.items()))\n",
    "plt.scatter( x= tmp_vol, y= tmp_ret, marker= '.', c='blue', label= 'CMA:long only+ concentration constrain')\n",
    "plt.scatter( x= minvar_portf2[1], y=minvar_portf2[0], marker= 'o', c= 'blue', s=100)\n",
    "plt.scatter(x= efficient_portf2[1], y = efficient_portf2[0], marker= '*', c='blue', s=200)\n",
    "# plt.plot( [0.04, efficient_portf2[1]], [ (efficient_portf2[0]-rf)/efficient_portf2[1]* 0.04+rf, efficient_portf2[0]], \n",
    "#          linestyle= '-', c= 'blue')\n",
    "\n",
    "\n",
    "[tmp_ret, tmp_vol]= zip( * list( fronter1_active_vol.items())) \n",
    "plt.scatter(x= tmp_vol, y= tmp_ret, marker=\".\", c= 'orange' , label= 'CMA_active:long only')\n",
    "plt.scatter(x= minvar_active_portf1[1], y= minvar_active_portf1[0], marker= 'o', c='orange', s= 100 )\n",
    "plt.scatter(x= efficient_portf1_active[1], y = efficient_portf1_active[0], marker= '*', c='orange', s=200)\n",
    "# plt.plot( [0.04, efficient_portf1_active[1]], [ (efficient_portf1_active[0]-rf)/efficient_portf1_active[1]* 0.04+rf, efficient_portf1_active[0]], \n",
    "#          linestyle='-', c='orange')\n",
    "\n",
    "[tmp_ret, tmp_vol]= zip(* list(fronter2_active_vol.items()))\n",
    "plt.scatter( x= tmp_vol, y= tmp_ret, marker= '.', c='red', label= 'CMA_active:long only+ concentration constrain')\n",
    "plt.scatter( x= minvar_active_portf2[1], y=minvar_active_portf2[0], marker= 'o', c= 'red', s=100)\n",
    "plt.scatter(x= efficient_portf2_active[1], y = efficient_portf2_active[0], marker= '*', c='red', s=200)\n",
    "# plt.plot( [0.04, efficient_portf2_active[1]], [ (efficient_portf2_active[0]-rf)/efficient_portf2_active[1]* 0.04+rf, efficient_portf2_active[0]], \n",
    "#          linestyle= '-', c='red')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Efficient Fronter')\n",
    "\n",
    "print(efficient_portf1)\n",
    "print(efficient_portf1_w)\n",
    "print(efficient_portf2)\n",
    "print(efficient_portf2_w)\n",
    "print(efficient_portf1_active)\n",
    "print(efficient_portf1_active_w)\n",
    "print(efficient_portf2_active)\n",
    "print(efficient_portf2_active_w)\n",
    "\n",
    "weight_longonly= efficient_portf1_w\n",
    "weight_longonly_conc= efficient_portf2_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_eq</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_peer</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>2.970297e-02</td>\n",
       "      <td>2.079208e-01</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_erc</th>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.142979</td>\n",
       "      <td>0.177851</td>\n",
       "      <td>0.118734</td>\n",
       "      <td>9.272408e-02</td>\n",
       "      <td>1.058735e-01</td>\n",
       "      <td>0.079423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_weight_longonly</th>\n",
       "      <td>0.576959</td>\n",
       "      <td>0.392526</td>\n",
       "      <td>0.015887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.517883e-18</td>\n",
       "      <td>5.285486e-19</td>\n",
       "      <td>0.014629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_weight_longonly_conc</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.180713</td>\n",
       "      <td>0.134196</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.117640e-02</td>\n",
       "      <td>0.013915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_active_weight_longonly</th>\n",
       "      <td>0.539164</td>\n",
       "      <td>0.312270</td>\n",
       "      <td>0.055479</td>\n",
       "      <td>0.057946</td>\n",
       "      <td>1.756746e-18</td>\n",
       "      <td>1.391612e-02</td>\n",
       "      <td>0.021225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_active_weight_longonly_conc</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.164172</td>\n",
       "      <td>0.140924</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.092334e-02</td>\n",
       "      <td>0.013980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    US_RE     US_PE     US_HY     SP500  \\\n",
       "weight_eq                        0.142857  0.142857  0.142857  0.142857   \n",
       "weight_peer                      0.138614  0.287129  0.049505  0.237624   \n",
       "weight_erc                       0.282415  0.142979  0.177851  0.118734   \n",
       "CMA_weight_longonly              0.576959  0.392526  0.015887  0.000000   \n",
       "CMA_weight_longonly_conc         0.250000  0.400000  0.180713  0.134196   \n",
       "CMA_active_weight_longonly       0.539164  0.312270  0.055479  0.057946   \n",
       "CMA_active_weight_longonly_conc  0.250000  0.400000  0.164172  0.140924   \n",
       "\n",
       "                                   Rusell2000          EAFE        EM  \n",
       "weight_eq                        1.428571e-01  1.428571e-01  0.142857  \n",
       "weight_peer                      2.970297e-02  2.079208e-01  0.049505  \n",
       "weight_erc                       9.272408e-02  1.058735e-01  0.079423  \n",
       "CMA_weight_longonly              1.517883e-18  5.285486e-19  0.014629  \n",
       "CMA_weight_longonly_conc         0.000000e+00  2.117640e-02  0.013915  \n",
       "CMA_active_weight_longonly       1.756746e-18  1.391612e-02  0.021225  \n",
       "CMA_active_weight_longonly_conc  0.000000e+00  3.092334e-02  0.013980  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portf_weight_3= pd.DataFrame( [efficient_portf1_active_w, efficient_portf2_active_w],\n",
    "                            index= ['CMA_active_weight_longonly', 'CMA_active_weight_longonly_conc'], \n",
    "                            columns= LW_cov.columns)\n",
    "\n",
    "pd.concat([portf_weight_1, portf_weight_2, portf_weight_3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0185311329096\n",
      "            Iterations: 23\n",
      "            Function evaluations: 24\n",
      "            Gradient evaluations: 23\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0186737278728\n",
      "            Iterations: 33\n",
      "            Function evaluations: 33\n",
      "            Gradient evaluations: 33\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0176003331126\n",
      "            Iterations: 22\n",
      "            Function evaluations: 22\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.017893465271\n",
      "            Iterations: 36\n",
      "            Function evaluations: 37\n",
      "            Gradient evaluations: 36\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0167160966106\n",
      "            Iterations: 21\n",
      "            Function evaluations: 21\n",
      "            Gradient evaluations: 21\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0171762350377\n",
      "            Iterations: 38\n",
      "            Function evaluations: 38\n",
      "            Gradient evaluations: 38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weight_CMAactive_MVO_gamma_3.0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.832755e-17</td>\n",
       "      <td>0.057108</td>\n",
       "      <td>0.141303</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>0.135674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_CMAactive_MVO_gamma_3.5</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.062577e-17</td>\n",
       "      <td>0.113080</td>\n",
       "      <td>0.100520</td>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.107229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_CMAactive_MVO_gamma_4.0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.422235e-02</td>\n",
       "      <td>0.145359</td>\n",
       "      <td>0.069217</td>\n",
       "      <td>0.036159</td>\n",
       "      <td>0.085043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                US_RE  US_PE         US_HY     SP500  \\\n",
       "weight_CMAactive_MVO_gamma_3.0   0.25    0.4  2.832755e-17  0.057108   \n",
       "weight_CMAactive_MVO_gamma_3.5   0.25    0.4  4.062577e-17  0.113080   \n",
       "weight_CMAactive_MVO_gamma_4.0   0.25    0.4  1.422235e-02  0.145359   \n",
       "\n",
       "                                Rusell2000      EAFE        EM  \n",
       "weight_CMAactive_MVO_gamma_3.0    0.141303  0.015915  0.135674  \n",
       "weight_CMAactive_MVO_gamma_3.5    0.100520  0.029170  0.107229  \n",
       "weight_CMAactive_MVO_gamma_4.0    0.069217  0.036159  0.085043  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Risk adj return utility constrained optimal with active management \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def obj_func_CMAactive(w, ARGS):  # ARGS= [sigma, ExpRet, gamma]\n",
    "    return (np.dot(  np.dot( w, ARGS[0]), w)* .5* ARGS[2]- np.dot( ARGS[1], w))\n",
    "\n",
    "def obj_func_derivative_CMAactive( w, ARGS): \n",
    "    return (np.dot( w, ARGS[0])* ARGS[2]- ARGS[1])\n",
    "\n",
    "\n",
    "cons_eq0= {'type': 'eq',\n",
    "          'fun': lambda w: np.array( [np.sum(w)-1] ) ,\n",
    "          'jac': lambda w: np.array( [1]*N)}\n",
    "# cons_ineq0= {'type': 'ineq', \n",
    "#              'fun': lambda w: -w[0]-w[1]+ 0.5, \n",
    "#              'jac': lambda w: np.array([-1]*2+ [0]*10)}\n",
    "# cons_ineq1= {'type': 'ineq',\n",
    "#              'fun': lambda w: -w[2]-w[3]-w[4]-w[5]-w[6]+ 0.5,\n",
    "#              'jac': lambda w: np.array([0]*2+ [-1]*5+ [0]*5)}\n",
    "# cons_ineq2= {'type': 'ineq', \n",
    "#              'fun': lambda w: -w[7]-w[8]-w[9]-w[10]+ 0.5, \n",
    "#              'jac': lambda w: np.array([0]*7+ [-1]*4+ [0])}\n",
    "# cons_ineq3= {'type': 'ineq', \n",
    "#             'fun': lambda w: -w[11]+ 0.5,\n",
    "#             'jac': lambda w: np.array( [0]*11+ [-1])}\n",
    "\n",
    "\n",
    "CMAactive_riskAdj_opt={}\n",
    "CMAactive_riskAdj_opt2={}\n",
    "for g in [3,3.5,4]: \n",
    "\n",
    "    cons= (cons_eq0\n",
    "    #        cons_ineq0,\n",
    "    #        cons_ineq1,\n",
    "    #        cons_ineq2,\n",
    "    #        cons_ineq3\n",
    "          )\n",
    "\n",
    "    MV_opt= minimize( obj_func_CMAactive, \n",
    "                    x0= weight_eq, \n",
    "                    args= [LW_cov_active, CMA_ExpRet_active_arith, g], \n",
    "                    jac= obj_func_derivative_CMAactive ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0, 0.25]]+[[0, .4]]+[[0,None]]* (N-2),\n",
    "                    tol= 1e-12)\n",
    "\n",
    "    MV_opt2= minimize( obj_func_CMAactive, \n",
    "                    x0= weight_eq, \n",
    "                    args= [LW_cov_active, CMA_ExpRet_active_arith, g], \n",
    "                    jac= obj_func_derivative_CMAactive ,\n",
    "                    method= 'SLSQP',\n",
    "                    options= {'disp': True},\n",
    "                    constraints= cons, \n",
    "                    bounds= [[0,None]]* N,\n",
    "                    tol= 1e-12)\n",
    "    \n",
    "    CMAactive_riskAdj_opt[g]= MV_opt.x\n",
    "    CMAactive_riskAdj_opt2[g]= MV_opt2.x\n",
    "    \n",
    "CMAactive_riskAdj_portf_w= pd.DataFrame( CMAactive_riskAdj_opt, index=LW_cov_active.columns).T\n",
    "CMAactive_riskAdj_portf_w.index= ['weight_CMAactive_MVO_gamma_'+str(x) for x in CMAactive_riskAdj_portf_w.index]\n",
    "CMAactive_riskAdj_portf_w2= pd.DataFrame( CMAactive_riskAdj_opt2, index= LW_cov_active.columns).T\n",
    "CMAactive_riskAdj_portf_w2.index= ['weight_CMAactive_MVO_gamma_'+str(x)+'uncons' for x in CMAactive_riskAdj_portf_w.index]\n",
    "CMAactive_riskAdj_portf_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-0ac1507400fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCMAactive_riskAdj_opt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "  pd.DataFrame(CMAactive_riskAdj_opt[2],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w_eq</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_peer</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>4.950495e-02</td>\n",
       "      <td>2.376238e-01</td>\n",
       "      <td>2.970297e-02</td>\n",
       "      <td>2.079208e-01</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_erc</th>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.142979</td>\n",
       "      <td>1.778511e-01</td>\n",
       "      <td>1.187341e-01</td>\n",
       "      <td>9.272408e-02</td>\n",
       "      <td>1.058735e-01</td>\n",
       "      <td>0.079423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_MVO_1</th>\n",
       "      <td>0.576959</td>\n",
       "      <td>0.392526</td>\n",
       "      <td>1.588694e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.517883e-18</td>\n",
       "      <td>5.285486e-19</td>\n",
       "      <td>0.014629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_MVO_2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.807130e-01</td>\n",
       "      <td>1.341957e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.117640e-02</td>\n",
       "      <td>0.013915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_RUO_3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.393563e-02</td>\n",
       "      <td>1.473398e-01</td>\n",
       "      <td>1.145913e-02</td>\n",
       "      <td>0.137265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_RUO_3.5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.185327e-01</td>\n",
       "      <td>9.978234e-02</td>\n",
       "      <td>2.314467e-02</td>\n",
       "      <td>0.108540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_RUO_4</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.077001e-02</td>\n",
       "      <td>1.363963e-01</td>\n",
       "      <td>6.369201e-02</td>\n",
       "      <td>2.457743e-02</td>\n",
       "      <td>0.084564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aMVO_1</th>\n",
       "      <td>0.539164</td>\n",
       "      <td>0.312270</td>\n",
       "      <td>5.547936e-02</td>\n",
       "      <td>5.794597e-02</td>\n",
       "      <td>1.756746e-18</td>\n",
       "      <td>1.391612e-02</td>\n",
       "      <td>0.021225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aMVO_2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.641722e-01</td>\n",
       "      <td>1.409241e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.092334e-02</td>\n",
       "      <td>0.013980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.832755e-17</td>\n",
       "      <td>5.710771e-02</td>\n",
       "      <td>1.413027e-01</td>\n",
       "      <td>1.591524e-02</td>\n",
       "      <td>0.135674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3.5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.062577e-17</td>\n",
       "      <td>1.130803e-01</td>\n",
       "      <td>1.005200e-01</td>\n",
       "      <td>2.917028e-02</td>\n",
       "      <td>0.107229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_4</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.422235e-02</td>\n",
       "      <td>1.453587e-01</td>\n",
       "      <td>6.921700e-02</td>\n",
       "      <td>3.615894e-02</td>\n",
       "      <td>0.085043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3unc</th>\n",
       "      <td>0.354512</td>\n",
       "      <td>0.410367</td>\n",
       "      <td>6.037227e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.032986e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.131823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3.5unc</th>\n",
       "      <td>0.408589</td>\n",
       "      <td>0.397578</td>\n",
       "      <td>1.737243e-17</td>\n",
       "      <td>8.192079e-18</td>\n",
       "      <td>8.344910e-02</td>\n",
       "      <td>1.182034e-17</td>\n",
       "      <td>0.110384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_4unc</th>\n",
       "      <td>0.448397</td>\n",
       "      <td>0.386824</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.910225e-03</td>\n",
       "      <td>6.679204e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.094077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  US_RE     US_PE         US_HY         SP500    Rusell2000  \\\n",
       "w_eq           0.142857  0.142857  1.428571e-01  1.428571e-01  1.428571e-01   \n",
       "w_peer         0.138614  0.287129  4.950495e-02  2.376238e-01  2.970297e-02   \n",
       "w_erc          0.282415  0.142979  1.778511e-01  1.187341e-01  9.272408e-02   \n",
       "w_MVO_1        0.576959  0.392526  1.588694e-02  0.000000e+00  1.517883e-18   \n",
       "w_MVO_2        0.250000  0.400000  1.807130e-01  1.341957e-01  0.000000e+00   \n",
       "w_RUO_3        0.250000  0.400000  0.000000e+00  5.393563e-02  1.473398e-01   \n",
       "w_RUO_3.5      0.250000  0.400000  0.000000e+00  1.185327e-01  9.978234e-02   \n",
       "w_RUO_4        0.250000  0.400000  4.077001e-02  1.363963e-01  6.369201e-02   \n",
       "w_aMVO_1       0.539164  0.312270  5.547936e-02  5.794597e-02  1.756746e-18   \n",
       "w_aMVO_2       0.250000  0.400000  1.641722e-01  1.409241e-01  0.000000e+00   \n",
       "w_aRUO_3       0.250000  0.400000  2.832755e-17  5.710771e-02  1.413027e-01   \n",
       "w_aRUO_3.5     0.250000  0.400000  4.062577e-17  1.130803e-01  1.005200e-01   \n",
       "w_aRUO_4       0.250000  0.400000  1.422235e-02  1.453587e-01  6.921700e-02   \n",
       "w_aRUO_3unc    0.354512  0.410367  6.037227e-19  0.000000e+00  1.032986e-01   \n",
       "w_aRUO_3.5unc  0.408589  0.397578  1.737243e-17  8.192079e-18  8.344910e-02   \n",
       "w_aRUO_4unc    0.448397  0.386824  0.000000e+00  3.910225e-03  6.679204e-02   \n",
       "\n",
       "                       EAFE        EM  \n",
       "w_eq           1.428571e-01  0.142857  \n",
       "w_peer         2.079208e-01  0.049505  \n",
       "w_erc          1.058735e-01  0.079423  \n",
       "w_MVO_1        5.285486e-19  0.014629  \n",
       "w_MVO_2        2.117640e-02  0.013915  \n",
       "w_RUO_3        1.145913e-02  0.137265  \n",
       "w_RUO_3.5      2.314467e-02  0.108540  \n",
       "w_RUO_4        2.457743e-02  0.084564  \n",
       "w_aMVO_1       1.391612e-02  0.021225  \n",
       "w_aMVO_2       3.092334e-02  0.013980  \n",
       "w_aRUO_3       1.591524e-02  0.135674  \n",
       "w_aRUO_3.5     2.917028e-02  0.107229  \n",
       "w_aRUO_4       3.615894e-02  0.085043  \n",
       "w_aRUO_3unc    0.000000e+00  0.131823  \n",
       "w_aRUO_3.5unc  1.182034e-17  0.110384  \n",
       "w_aRUO_4unc    0.000000e+00  0.094077  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portf_weight= pd.concat( [portf_weight_1, \n",
    "                          portf_weight_2, \n",
    "                          CMA_riskAdj_portf_w.iloc[ -3:] ,\n",
    "                          portf_weight_3, \n",
    "                          CMAactive_riskAdj_portf_w,\n",
    "                         CMAactive_riskAdj_portf_w2], axis= 0 )\n",
    "portf_weight.index= ['w_eq', # equal weight \n",
    "                    'w_peer', # peer weight\n",
    "                    'w_erc', # equal risk contribution weight \n",
    "                    'w_MVO_1', # mean-variance optimal weight with long only constrain\n",
    "                    'w_MVO_2', # mean-variance optimal weight with long only+ concentration constrain\n",
    "                    'w_RUO_3', # risk adj utility optimal weight with long only+ concentration constrain, give risk aversion 3 \n",
    "                    'w_RUO_3.5', # risk adj utility optimal weight with long only+ concentration constrain, given risk aversion 3.5\n",
    "                    'w_RUO_4', # risk adj utility optimal weight with long only+ concentration constrain, give risk aversion 4\n",
    "                    'w_aMVO_1', # mean-variance optimal weight with long only constrain, and active management \n",
    "                    'w_aMVO_2', # mean-variance optimal weight with long only+ concentration constrain, and active management \n",
    "                    'w_aRUO_3', # risk adj utility optimal weight with long only+ concentration constrain, and active management, give risk aversion 3 \n",
    "                    'w_aRUO_3.5', # risk adj utility optimal weight with long only+ concentration constrain, and active management, give risk aversion 3.5 \n",
    "                    'w_aRUO_4', # risk adj utility optimal weight with long only+ concentration constrain, and active management, give risk aversion 4\n",
    "                    'w_aRUO_3unc',\n",
    "                    'w_aRUO_3.5unc',\n",
    "                    'w_aRUO_4unc'] \n",
    "portf_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iRet_eq_3</th>\n",
       "      <td>2.946497</td>\n",
       "      <td>5.112730</td>\n",
       "      <td>4.197299</td>\n",
       "      <td>5.940288</td>\n",
       "      <td>6.745272</td>\n",
       "      <td>6.198200</td>\n",
       "      <td>6.975123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_eq_3.5</th>\n",
       "      <td>3.223899</td>\n",
       "      <td>5.834154</td>\n",
       "      <td>4.742391</td>\n",
       "      <td>6.827058</td>\n",
       "      <td>7.894844</td>\n",
       "      <td>7.209535</td>\n",
       "      <td>8.370075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_eq_4</th>\n",
       "      <td>3.501302</td>\n",
       "      <td>6.555578</td>\n",
       "      <td>5.287484</td>\n",
       "      <td>7.713828</td>\n",
       "      <td>9.044416</td>\n",
       "      <td>8.220869</td>\n",
       "      <td>9.765026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3</th>\n",
       "      <td>2.875835</td>\n",
       "      <td>5.296642</td>\n",
       "      <td>3.655122</td>\n",
       "      <td>5.827752</td>\n",
       "      <td>6.117100</td>\n",
       "      <td>6.102522</td>\n",
       "      <td>5.785744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3.5</th>\n",
       "      <td>3.141460</td>\n",
       "      <td>6.048718</td>\n",
       "      <td>4.109852</td>\n",
       "      <td>6.695766</td>\n",
       "      <td>7.161977</td>\n",
       "      <td>7.097910</td>\n",
       "      <td>6.982466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_4</th>\n",
       "      <td>3.407086</td>\n",
       "      <td>6.800794</td>\n",
       "      <td>4.564582</td>\n",
       "      <td>7.563779</td>\n",
       "      <td>8.206854</td>\n",
       "      <td>8.093298</td>\n",
       "      <td>8.179187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_3</th>\n",
       "      <td>3.116898</td>\n",
       "      <td>4.408355</td>\n",
       "      <td>3.840304</td>\n",
       "      <td>4.983875</td>\n",
       "      <td>5.436244</td>\n",
       "      <td>5.024522</td>\n",
       "      <td>5.129680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_3.5</th>\n",
       "      <td>3.422701</td>\n",
       "      <td>5.012383</td>\n",
       "      <td>4.325897</td>\n",
       "      <td>5.711243</td>\n",
       "      <td>6.367644</td>\n",
       "      <td>5.840244</td>\n",
       "      <td>6.217058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_4</th>\n",
       "      <td>3.728504</td>\n",
       "      <td>5.616411</td>\n",
       "      <td>4.811491</td>\n",
       "      <td>6.438610</td>\n",
       "      <td>7.299045</td>\n",
       "      <td>6.655965</td>\n",
       "      <td>7.304435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>7.210000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>7.070000</td>\n",
       "      <td>8.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_active</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>9.060000</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>9.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "iRet_eq_3      2.946497  5.112730  4.197299  5.940288    6.745272  6.198200   \n",
       "iRet_eq_3.5    3.223899  5.834154  4.742391  6.827058    7.894844  7.209535   \n",
       "iRet_eq_4      3.501302  6.555578  5.287484  7.713828    9.044416  8.220869   \n",
       "iRet_peer_3    2.875835  5.296642  3.655122  5.827752    6.117100  6.102522   \n",
       "iRet_peer_3.5  3.141460  6.048718  4.109852  6.695766    7.161977  7.097910   \n",
       "iRet_peer_4    3.407086  6.800794  4.564582  7.563779    8.206854  8.093298   \n",
       "iRet_erc_3     3.116898  4.408355  3.840304  4.983875    5.436244  5.024522   \n",
       "iRet_erc_3.5   3.422701  5.012383  4.325897  5.711243    6.367644  5.840244   \n",
       "iRet_erc_4     3.728504  5.616411  4.811491  6.438610    7.299045  6.655965   \n",
       "CMA            7.000000  8.800000  4.770000  7.210000    8.060000  7.070000   \n",
       "CMA_active     7.000000  8.800000  5.520000  8.210000    9.060000  8.070000   \n",
       "\n",
       "                     EM  \n",
       "iRet_eq_3      6.975123  \n",
       "iRet_eq_3.5    8.370075  \n",
       "iRet_eq_4      9.765026  \n",
       "iRet_peer_3    5.785744  \n",
       "iRet_peer_3.5  6.982466  \n",
       "iRet_peer_4    8.179187  \n",
       "iRet_erc_3     5.129680  \n",
       "iRet_erc_3.5   6.217058  \n",
       "iRet_erc_4     7.304435  \n",
       "CMA            8.030000  \n",
       "CMA_active     9.030000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpRet=  pd.concat([implied_ExpRet['weight_eq'][['3_shrunk', '3.5_shrunk', '4_shrunk']].T, \n",
    "                   implied_ExpRet['weight_peer'][['3_shrunk', '3.5_shrunk', '4_shrunk']].T,\n",
    "                   implied_ExpRet['weight_erc'][['3_shrunk', '3.5_shrunk', '4_shrunk']].T,\n",
    "                   pd.DataFrame([CMA_ExpRet_geo,CMA_ExpRet_active_geo], columns= LW_cov.columns)], \n",
    "                   axis=0)* 400\n",
    "ExpRet.index= ['iRet_eq_3', # implied expected return from equal weight with risk aversion 3\n",
    "               'iRet_eq_3.5', # implied expected return from equal weight with risk aversion 3.5\n",
    "               'iRet_eq_4',\n",
    "               'iRet_peer_3', # implied expected return from peer weight with risk aversion 3\n",
    "               'iRet_peer_3.5', # implied expected return from peer weight with risk aversion 3.5\n",
    "               'iRet_peer_4',\n",
    "               'iRet_erc_3', # implied expected return from equal risk contribution weight with risk aversion 3\n",
    "               'iRet_erc_3.5',# implied expected return from equal risk contribution weight with risk aversion 3.5\n",
    "               'iRet_erc_4',\n",
    "               'CMA', # CMA expected return \n",
    "               'CMA_active' # CMA expected return, and active management\n",
    "              ]\n",
    "\n",
    "ExpRet # annualized expected ret in percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Black-Litterman Framework \n",
    "\n",
    "Construct BL framework to incorporate benchmark(prior) and views(observations) and produce a reasonable distribution of expected return (posterior). \n",
    "Apply mean-variance optimization based on posterior to achieve optimal allocation. \n",
    "\n",
    "#### Benckmark/Equilibrium Portfolio\n",
    "\n",
    "Set the benchmark as peer holding `w_peer`, then `iRet_peer_3.5` is the implied equilibrium\\benchmark expected return, given risk aversion factor 4.\n",
    "\n",
    "#### The prior confidence  $\\tau$\n",
    "\n",
    "Follow BL's initial setting, $\\tau = 0.05$\n",
    "\n",
    "#### Views\n",
    "\n",
    "`CMA_active` is the subjective view to expected return of each asset. The confidence is proportional to view portfolio (prior) variance with multiplier $\\tau$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### As summary, input: \n",
    "\n",
    "$\\tau$\n",
    "\n",
    "prior expected ret distribution, assuming normal, so the prior mean and variance \n",
    "\n",
    "views, the view portfolio weight, asserted expected ret, and view confidence. \n",
    "\n",
    "#### output: \n",
    "\n",
    "the posterior distribution, mean and variance of post expected return. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## prepare input\n",
    "\n",
    "tau = 5e-2\n",
    "prior_cov= LW_cov* tau\n",
    "prior_cov_inv= np.linalg.inv(prior_cov)\n",
    "prior_mean= np.array(ExpRet.loc['iRet_peer_3.5'].tolist())/100/4+ 0.5* np.diag(LW_cov)\n",
    "\n",
    "\n",
    "\n",
    "# CMA_ExpRet_active_arith \n",
    "# is the asserted expected return \n",
    "view_w= np.identity(N)\n",
    "view_ExpRet= CMA_ExpRet_active_arith\n",
    "view_cov= ( (LW_cov_active)* tau)\n",
    "view_cov_inv= np.linalg.inv( view_cov)\n",
    "\n",
    "##  output: post \n",
    "\n",
    "A= prior_cov_inv\n",
    "B= np.dot( np.dot(view_w.T, view_cov_inv), view_w)\n",
    "C= np.dot(prior_cov_inv, prior_mean)\n",
    "D= np.dot(np.dot(view_w.T, view_cov_inv), view_ExpRet)\n",
    "\n",
    "post_mean_arith= pd.DataFrame( np.dot(np.linalg.inv( A+B), C+D), index=LW_cov.index, columns= ['post_ExpRet']) .T\n",
    "post_cov= pd.DataFrame( np.linalg.inv( prior_cov_inv+ np.dot( np.dot( view_w.T, view_cov_inv), view_w)), index= LW_cov.index, columns= LW_cov.columns)\n",
    "post_mean_geo= post_mean_arith- .5* np.diag( LW_cov_active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3.5</th>\n",
       "      <td>3.14146</td>\n",
       "      <td>6.048718</td>\n",
       "      <td>4.109852</td>\n",
       "      <td>6.695766</td>\n",
       "      <td>7.161977</td>\n",
       "      <td>7.097910</td>\n",
       "      <td>6.982466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_active</th>\n",
       "      <td>7.00000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>9.060000</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>9.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_ExpRet</th>\n",
       "      <td>5.07073</td>\n",
       "      <td>7.424359</td>\n",
       "      <td>4.785615</td>\n",
       "      <td>7.433425</td>\n",
       "      <td>8.097552</td>\n",
       "      <td>7.575155</td>\n",
       "      <td>7.983203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "iRet_peer_3.5  3.14146  6.048718  4.109852  6.695766    7.161977  7.097910   \n",
       "CMA_active     7.00000  8.800000  5.520000  8.210000    9.060000  8.070000   \n",
       "post_ExpRet    5.07073  7.424359  4.785615  7.433425    8.097552  7.575155   \n",
       "\n",
       "                     EM  \n",
       "iRet_peer_3.5  6.982466  \n",
       "CMA_active     9.030000  \n",
       "post_ExpRet    7.983203  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([ ExpRet.loc[['iRet_peer_3.5', 'CMA_active']], \n",
    "          post_mean_geo*400], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "US_RE       0.000063  0.000026  0.000020  0.000029    0.000038  0.000029   \n",
       "US_PE       0.000026  0.000126  0.000044  0.000093    0.000117  0.000098   \n",
       "US_HY       0.000020  0.000044  0.000111  0.000063    0.000079  0.000068   \n",
       "SP500       0.000029  0.000093  0.000063  0.000149    0.000157  0.000133   \n",
       "Rusell2000  0.000038  0.000117  0.000079  0.000157    0.000245  0.000157   \n",
       "EAFE        0.000029  0.000098  0.000068  0.000133    0.000157  0.000210   \n",
       "EM          0.000037  0.000128  0.000095  0.000156    0.000215  0.000192   \n",
       "\n",
       "                  EM  \n",
       "US_RE       0.000037  \n",
       "US_PE       0.000128  \n",
       "US_HY       0.000095  \n",
       "SP500       0.000156  \n",
       "Rusell2000  0.000215  \n",
       "EAFE        0.000192  \n",
       "EM          0.000401  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive directional derivative for linesearch    (Exit mode 8)\n",
      "            Current function value: [-0.01326817]\n",
      "            Iterations: 42\n",
      "            Function evaluations: 38\n",
      "            Gradient evaluations: 38\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.25      ,  0.38859788,  0.02774285,  0.14274557,  0.0337973 ,\n",
       "        0.10022577,  0.05689063])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MVO based over post expected mean and variance\n",
    "\n",
    "\n",
    "MV_opt= minimize( obj_func_CMA, \n",
    "                x0= weight_eq, \n",
    "                args= [LW_cov_active+ post_cov, post_mean_arith, 4], \n",
    "                jac= obj_func_derivative_CMA ,\n",
    "                method= 'SLSQP',\n",
    "                options= {'disp': True},\n",
    "                constraints= cons, \n",
    "                bounds= [[0, 0.25]]+[[0, .4]]+[[0,None]]* (N-2),\n",
    "                tol= 1e-120)\n",
    "\n",
    "MV_opt.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: [-0.01335691]\n",
      "            Iterations: 43\n",
      "            Function evaluations: 54\n",
      "            Gradient evaluations: 43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  3.48519629e-01,   3.38257530e-01,   8.32631870e-27,\n",
       "         1.13383315e-01,   4.04834854e-02,   9.43138625e-02,\n",
       "         6.50421785e-02])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MV_unc_opt= minimize( obj_func_CMA, \n",
    "                x0= weight_eq, \n",
    "                args= [LW_cov_active+ post_cov, post_mean_arith, 4], \n",
    "                jac= obj_func_derivative_CMA ,\n",
    "                method= 'SLSQP',\n",
    "                options= {'disp': True},\n",
    "                constraints= cons, \n",
    "                bounds= [[0,None]]* N,\n",
    "                tol= 1e-120)\n",
    "\n",
    "MV_unc_opt.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w_eq</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>1.428571e-01</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_peer</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>4.950495e-02</td>\n",
       "      <td>2.376238e-01</td>\n",
       "      <td>2.970297e-02</td>\n",
       "      <td>2.079208e-01</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_erc</th>\n",
       "      <td>0.282415</td>\n",
       "      <td>0.142979</td>\n",
       "      <td>1.778511e-01</td>\n",
       "      <td>1.187341e-01</td>\n",
       "      <td>9.272408e-02</td>\n",
       "      <td>1.058735e-01</td>\n",
       "      <td>0.079423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_MVO_1</th>\n",
       "      <td>0.576959</td>\n",
       "      <td>0.392526</td>\n",
       "      <td>1.588694e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.517883e-18</td>\n",
       "      <td>5.285486e-19</td>\n",
       "      <td>0.014629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_MVO_2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.807130e-01</td>\n",
       "      <td>1.341957e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.117640e-02</td>\n",
       "      <td>0.013915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_RUO_3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.393563e-02</td>\n",
       "      <td>1.473398e-01</td>\n",
       "      <td>1.145913e-02</td>\n",
       "      <td>0.137265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_RUO_3.5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.185327e-01</td>\n",
       "      <td>9.978234e-02</td>\n",
       "      <td>2.314467e-02</td>\n",
       "      <td>0.108540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_RUO_4</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.077001e-02</td>\n",
       "      <td>1.363963e-01</td>\n",
       "      <td>6.369201e-02</td>\n",
       "      <td>2.457743e-02</td>\n",
       "      <td>0.084564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aMVO_1</th>\n",
       "      <td>0.539164</td>\n",
       "      <td>0.312270</td>\n",
       "      <td>5.547936e-02</td>\n",
       "      <td>5.794597e-02</td>\n",
       "      <td>1.756746e-18</td>\n",
       "      <td>1.391612e-02</td>\n",
       "      <td>0.021225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aMVO_2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.641722e-01</td>\n",
       "      <td>1.409241e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.092334e-02</td>\n",
       "      <td>0.013980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.832755e-17</td>\n",
       "      <td>5.710771e-02</td>\n",
       "      <td>1.413027e-01</td>\n",
       "      <td>1.591524e-02</td>\n",
       "      <td>0.135674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3.5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.062577e-17</td>\n",
       "      <td>1.130803e-01</td>\n",
       "      <td>1.005200e-01</td>\n",
       "      <td>2.917028e-02</td>\n",
       "      <td>0.107229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_4</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.422235e-02</td>\n",
       "      <td>1.453587e-01</td>\n",
       "      <td>6.921700e-02</td>\n",
       "      <td>3.615894e-02</td>\n",
       "      <td>0.085043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3unc</th>\n",
       "      <td>0.354512</td>\n",
       "      <td>0.410367</td>\n",
       "      <td>6.037227e-19</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.032986e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.131823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3.5unc</th>\n",
       "      <td>0.408589</td>\n",
       "      <td>0.397578</td>\n",
       "      <td>1.737243e-17</td>\n",
       "      <td>8.192079e-18</td>\n",
       "      <td>8.344910e-02</td>\n",
       "      <td>1.182034e-17</td>\n",
       "      <td>0.110384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_4unc</th>\n",
       "      <td>0.448397</td>\n",
       "      <td>0.386824</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.910225e-03</td>\n",
       "      <td>6.679204e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.094077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_BL</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.388598</td>\n",
       "      <td>2.774285e-02</td>\n",
       "      <td>1.427456e-01</td>\n",
       "      <td>3.379730e-02</td>\n",
       "      <td>1.002258e-01</td>\n",
       "      <td>0.056891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_BL_unc</th>\n",
       "      <td>0.348520</td>\n",
       "      <td>0.338258</td>\n",
       "      <td>8.326319e-27</td>\n",
       "      <td>1.133833e-01</td>\n",
       "      <td>4.048349e-02</td>\n",
       "      <td>9.431386e-02</td>\n",
       "      <td>0.065042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  US_RE     US_PE         US_HY         SP500    Rusell2000  \\\n",
       "w_eq           0.142857  0.142857  1.428571e-01  1.428571e-01  1.428571e-01   \n",
       "w_peer         0.138614  0.287129  4.950495e-02  2.376238e-01  2.970297e-02   \n",
       "w_erc          0.282415  0.142979  1.778511e-01  1.187341e-01  9.272408e-02   \n",
       "w_MVO_1        0.576959  0.392526  1.588694e-02  0.000000e+00  1.517883e-18   \n",
       "w_MVO_2        0.250000  0.400000  1.807130e-01  1.341957e-01  0.000000e+00   \n",
       "w_RUO_3        0.250000  0.400000  0.000000e+00  5.393563e-02  1.473398e-01   \n",
       "w_RUO_3.5      0.250000  0.400000  0.000000e+00  1.185327e-01  9.978234e-02   \n",
       "w_RUO_4        0.250000  0.400000  4.077001e-02  1.363963e-01  6.369201e-02   \n",
       "w_aMVO_1       0.539164  0.312270  5.547936e-02  5.794597e-02  1.756746e-18   \n",
       "w_aMVO_2       0.250000  0.400000  1.641722e-01  1.409241e-01  0.000000e+00   \n",
       "w_aRUO_3       0.250000  0.400000  2.832755e-17  5.710771e-02  1.413027e-01   \n",
       "w_aRUO_3.5     0.250000  0.400000  4.062577e-17  1.130803e-01  1.005200e-01   \n",
       "w_aRUO_4       0.250000  0.400000  1.422235e-02  1.453587e-01  6.921700e-02   \n",
       "w_aRUO_3unc    0.354512  0.410367  6.037227e-19  0.000000e+00  1.032986e-01   \n",
       "w_aRUO_3.5unc  0.408589  0.397578  1.737243e-17  8.192079e-18  8.344910e-02   \n",
       "w_aRUO_4unc    0.448397  0.386824  0.000000e+00  3.910225e-03  6.679204e-02   \n",
       "w_BL           0.250000  0.388598  2.774285e-02  1.427456e-01  3.379730e-02   \n",
       "w_BL_unc       0.348520  0.338258  8.326319e-27  1.133833e-01  4.048349e-02   \n",
       "\n",
       "                       EAFE        EM  \n",
       "w_eq           1.428571e-01  0.142857  \n",
       "w_peer         2.079208e-01  0.049505  \n",
       "w_erc          1.058735e-01  0.079423  \n",
       "w_MVO_1        5.285486e-19  0.014629  \n",
       "w_MVO_2        2.117640e-02  0.013915  \n",
       "w_RUO_3        1.145913e-02  0.137265  \n",
       "w_RUO_3.5      2.314467e-02  0.108540  \n",
       "w_RUO_4        2.457743e-02  0.084564  \n",
       "w_aMVO_1       1.391612e-02  0.021225  \n",
       "w_aMVO_2       3.092334e-02  0.013980  \n",
       "w_aRUO_3       1.591524e-02  0.135674  \n",
       "w_aRUO_3.5     2.917028e-02  0.107229  \n",
       "w_aRUO_4       3.615894e-02  0.085043  \n",
       "w_aRUO_3unc    0.000000e+00  0.131823  \n",
       "w_aRUO_3.5unc  1.182034e-17  0.110384  \n",
       "w_aRUO_4unc    0.000000e+00  0.094077  \n",
       "w_BL           1.002258e-01  0.056891  \n",
       "w_BL_unc       9.431386e-02  0.065042  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portf_weight_final= pd.concat( [portf_weight,\n",
    "                         pd.DataFrame( [MV_opt.x, MV_unc_opt.x], columns= portf_weight.columns, index= ['w_BL', 'w_BL_unc'])], axis=0)\n",
    "portf_weight_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w_peer</th>\n",
       "      <td>0.138614</td>\n",
       "      <td>0.287129</td>\n",
       "      <td>4.950495e-02</td>\n",
       "      <td>0.237624</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.207921</td>\n",
       "      <td>0.049505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_aRUO_3.5</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.062577e-17</td>\n",
       "      <td>0.113080</td>\n",
       "      <td>0.100520</td>\n",
       "      <td>0.029170</td>\n",
       "      <td>0.107229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_BL</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.388598</td>\n",
       "      <td>2.774285e-02</td>\n",
       "      <td>0.142746</td>\n",
       "      <td>0.033797</td>\n",
       "      <td>0.100226</td>\n",
       "      <td>0.056891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_BL_unc</th>\n",
       "      <td>0.348520</td>\n",
       "      <td>0.338258</td>\n",
       "      <td>8.326319e-27</td>\n",
       "      <td>0.113383</td>\n",
       "      <td>0.040483</td>\n",
       "      <td>0.094314</td>\n",
       "      <td>0.065042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               US_RE     US_PE         US_HY     SP500  Rusell2000      EAFE  \\\n",
       "w_peer      0.138614  0.287129  4.950495e-02  0.237624    0.029703  0.207921   \n",
       "w_aRUO_3.5  0.250000  0.400000  4.062577e-17  0.113080    0.100520  0.029170   \n",
       "w_BL        0.250000  0.388598  2.774285e-02  0.142746    0.033797  0.100226   \n",
       "w_BL_unc    0.348520  0.338258  8.326319e-27  0.113383    0.040483  0.094314   \n",
       "\n",
       "                  EM  \n",
       "w_peer      0.049505  \n",
       "w_aRUO_3.5  0.107229  \n",
       "w_BL        0.056891  \n",
       "w_BL_unc    0.065042  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "portf_weight_final.loc[['w_peer', 'w_aRUO_3.5', 'w_BL', 'w_BL_unc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iRet_eq_3</th>\n",
       "      <td>2.946497</td>\n",
       "      <td>5.112730</td>\n",
       "      <td>4.197299</td>\n",
       "      <td>5.940288</td>\n",
       "      <td>6.745272</td>\n",
       "      <td>6.198200</td>\n",
       "      <td>6.975123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_eq_3.5</th>\n",
       "      <td>3.223899</td>\n",
       "      <td>5.834154</td>\n",
       "      <td>4.742391</td>\n",
       "      <td>6.827058</td>\n",
       "      <td>7.894844</td>\n",
       "      <td>7.209535</td>\n",
       "      <td>8.370075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_eq_4</th>\n",
       "      <td>3.501302</td>\n",
       "      <td>6.555578</td>\n",
       "      <td>5.287484</td>\n",
       "      <td>7.713828</td>\n",
       "      <td>9.044416</td>\n",
       "      <td>8.220869</td>\n",
       "      <td>9.765026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3</th>\n",
       "      <td>2.875835</td>\n",
       "      <td>5.296642</td>\n",
       "      <td>3.655122</td>\n",
       "      <td>5.827752</td>\n",
       "      <td>6.117100</td>\n",
       "      <td>6.102522</td>\n",
       "      <td>5.785744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3.5</th>\n",
       "      <td>3.141460</td>\n",
       "      <td>6.048718</td>\n",
       "      <td>4.109852</td>\n",
       "      <td>6.695766</td>\n",
       "      <td>7.161977</td>\n",
       "      <td>7.097910</td>\n",
       "      <td>6.982466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_4</th>\n",
       "      <td>3.407086</td>\n",
       "      <td>6.800794</td>\n",
       "      <td>4.564582</td>\n",
       "      <td>7.563779</td>\n",
       "      <td>8.206854</td>\n",
       "      <td>8.093298</td>\n",
       "      <td>8.179187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_3</th>\n",
       "      <td>3.116898</td>\n",
       "      <td>4.408355</td>\n",
       "      <td>3.840304</td>\n",
       "      <td>4.983875</td>\n",
       "      <td>5.436244</td>\n",
       "      <td>5.024522</td>\n",
       "      <td>5.129680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_3.5</th>\n",
       "      <td>3.422701</td>\n",
       "      <td>5.012383</td>\n",
       "      <td>4.325897</td>\n",
       "      <td>5.711243</td>\n",
       "      <td>6.367644</td>\n",
       "      <td>5.840244</td>\n",
       "      <td>6.217058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_4</th>\n",
       "      <td>3.728504</td>\n",
       "      <td>5.616411</td>\n",
       "      <td>4.811491</td>\n",
       "      <td>6.438610</td>\n",
       "      <td>7.299045</td>\n",
       "      <td>6.655965</td>\n",
       "      <td>7.304435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>7.210000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>7.070000</td>\n",
       "      <td>8.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_active</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>9.060000</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>9.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_ExpRet</th>\n",
       "      <td>5.070730</td>\n",
       "      <td>7.424359</td>\n",
       "      <td>4.785615</td>\n",
       "      <td>7.433425</td>\n",
       "      <td>8.097552</td>\n",
       "      <td>7.575155</td>\n",
       "      <td>7.983203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "iRet_eq_3      2.946497  5.112730  4.197299  5.940288    6.745272  6.198200   \n",
       "iRet_eq_3.5    3.223899  5.834154  4.742391  6.827058    7.894844  7.209535   \n",
       "iRet_eq_4      3.501302  6.555578  5.287484  7.713828    9.044416  8.220869   \n",
       "iRet_peer_3    2.875835  5.296642  3.655122  5.827752    6.117100  6.102522   \n",
       "iRet_peer_3.5  3.141460  6.048718  4.109852  6.695766    7.161977  7.097910   \n",
       "iRet_peer_4    3.407086  6.800794  4.564582  7.563779    8.206854  8.093298   \n",
       "iRet_erc_3     3.116898  4.408355  3.840304  4.983875    5.436244  5.024522   \n",
       "iRet_erc_3.5   3.422701  5.012383  4.325897  5.711243    6.367644  5.840244   \n",
       "iRet_erc_4     3.728504  5.616411  4.811491  6.438610    7.299045  6.655965   \n",
       "CMA            7.000000  8.800000  4.770000  7.210000    8.060000  7.070000   \n",
       "CMA_active     7.000000  8.800000  5.520000  8.210000    9.060000  8.070000   \n",
       "post_ExpRet    5.070730  7.424359  4.785615  7.433425    8.097552  7.575155   \n",
       "\n",
       "                     EM  \n",
       "iRet_eq_3      6.975123  \n",
       "iRet_eq_3.5    8.370075  \n",
       "iRet_eq_4      9.765026  \n",
       "iRet_peer_3    5.785744  \n",
       "iRet_peer_3.5  6.982466  \n",
       "iRet_peer_4    8.179187  \n",
       "iRet_erc_3     5.129680  \n",
       "iRet_erc_3.5   6.217058  \n",
       "iRet_erc_4     7.304435  \n",
       "CMA            8.030000  \n",
       "CMA_active     9.030000  \n",
       "post_ExpRet    7.983203  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpRet_final= pd.concat([ExpRet, \n",
    "                        post_mean_geo*400], axis=0)\n",
    "ExpRet_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100789</td>\n",
       "      <td>0.141832</td>\n",
       "      <td>0.131397</td>\n",
       "      <td>0.152992</td>\n",
       "      <td>0.197087</td>\n",
       "      <td>0.182198</td>\n",
       "      <td>0.252372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6\n",
       "0  0.100789  0.141832  0.131397  0.152992  0.197087  0.182198  0.252372"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.sqrt(np.diag( LW_cov))).T*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.290357</td>\n",
       "      <td>0.243449</td>\n",
       "      <td>0.301545</td>\n",
       "      <td>0.307428</td>\n",
       "      <td>0.255969</td>\n",
       "      <td>0.229836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>0.290357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.377413</td>\n",
       "      <td>0.682977</td>\n",
       "      <td>0.671396</td>\n",
       "      <td>0.604658</td>\n",
       "      <td>0.572320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>0.243449</td>\n",
       "      <td>0.377413</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.497447</td>\n",
       "      <td>0.488964</td>\n",
       "      <td>0.455539</td>\n",
       "      <td>0.459033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.301545</td>\n",
       "      <td>0.682977</td>\n",
       "      <td>0.497447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832526</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.644399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>0.307428</td>\n",
       "      <td>0.671396</td>\n",
       "      <td>0.488964</td>\n",
       "      <td>0.832526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.698464</td>\n",
       "      <td>0.691124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>0.255969</td>\n",
       "      <td>0.604658</td>\n",
       "      <td>0.455539</td>\n",
       "      <td>0.763682</td>\n",
       "      <td>0.698464</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.669628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.229836</td>\n",
       "      <td>0.572320</td>\n",
       "      <td>0.459033</td>\n",
       "      <td>0.644399</td>\n",
       "      <td>0.691124</td>\n",
       "      <td>0.669628</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "US_RE       1.000000  0.290357  0.243449  0.301545    0.307428  0.255969   \n",
       "US_PE       0.290357  1.000000  0.377413  0.682977    0.671396  0.604658   \n",
       "US_HY       0.243449  0.377413  1.000000  0.497447    0.488964  0.455539   \n",
       "SP500       0.301545  0.682977  0.497447  1.000000    0.832526  0.763682   \n",
       "Rusell2000  0.307428  0.671396  0.488964  0.832526    1.000000  0.698464   \n",
       "EAFE        0.255969  0.604658  0.455539  0.763682    0.698464  1.000000   \n",
       "EM          0.229836  0.572320  0.459033  0.644399    0.691124  0.669628   \n",
       "\n",
       "                  EM  \n",
       "US_RE       0.229836  \n",
       "US_PE       0.572320  \n",
       "US_HY       0.459033  \n",
       "SP500       0.644399  \n",
       "Rusell2000  0.691124  \n",
       "EAFE        0.669628  \n",
       "EM          1.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LW_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.290357</td>\n",
       "      <td>0.237342</td>\n",
       "      <td>0.295909</td>\n",
       "      <td>0.303927</td>\n",
       "      <td>0.252569</td>\n",
       "      <td>0.228229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>0.290357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.367945</td>\n",
       "      <td>0.670214</td>\n",
       "      <td>0.663750</td>\n",
       "      <td>0.596624</td>\n",
       "      <td>0.568319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>0.237342</td>\n",
       "      <td>0.367945</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.475904</td>\n",
       "      <td>0.471268</td>\n",
       "      <td>0.438210</td>\n",
       "      <td>0.444389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.295909</td>\n",
       "      <td>0.670214</td>\n",
       "      <td>0.475904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807664</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>0.627936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>0.303927</td>\n",
       "      <td>0.663750</td>\n",
       "      <td>0.471268</td>\n",
       "      <td>0.807664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.681336</td>\n",
       "      <td>0.678477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>0.252569</td>\n",
       "      <td>0.596624</td>\n",
       "      <td>0.438210</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>0.681336</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.656112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.228229</td>\n",
       "      <td>0.568319</td>\n",
       "      <td>0.444389</td>\n",
       "      <td>0.627936</td>\n",
       "      <td>0.678477</td>\n",
       "      <td>0.656112</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "US_RE       1.000000  0.290357  0.237342  0.295909    0.303927  0.252569   \n",
       "US_PE       0.290357  1.000000  0.367945  0.670214    0.663750  0.596624   \n",
       "US_HY       0.237342  0.367945  1.000000  0.475904    0.471268  0.438210   \n",
       "SP500       0.295909  0.670214  0.475904  1.000000    0.807664  0.739453   \n",
       "Rusell2000  0.303927  0.663750  0.471268  0.807664    1.000000  0.681336   \n",
       "EAFE        0.252569  0.596624  0.438210  0.739453    0.681336  1.000000   \n",
       "EM          0.228229  0.568319  0.444389  0.627936    0.678477  0.656112   \n",
       "\n",
       "                  EM  \n",
       "US_RE       0.228229  \n",
       "US_PE       0.568319  \n",
       "US_HY       0.444389  \n",
       "SP500       0.627936  \n",
       "Rusell2000  0.678477  \n",
       "EAFE        0.656112  \n",
       "EM          1.000000  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LW_corr_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.130784</td>\n",
       "      <td>0.124802</td>\n",
       "      <td>0.109958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.130784  0.124802  0.109958"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.sqrt(np.diag(np.dot(np.dot(portf_weight.iloc[0:3], LW_cov), portf_weight.iloc[0:3].T)))*2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>US_RE</th>\n",
       "      <th>US_PE</th>\n",
       "      <th>US_HY</th>\n",
       "      <th>SP500</th>\n",
       "      <th>Rusell2000</th>\n",
       "      <th>EAFE</th>\n",
       "      <th>EM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iRet_eq_3</th>\n",
       "      <td>2.946497</td>\n",
       "      <td>5.112730</td>\n",
       "      <td>4.197299</td>\n",
       "      <td>5.940288</td>\n",
       "      <td>6.745272</td>\n",
       "      <td>6.198200</td>\n",
       "      <td>6.975123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_eq_3.5</th>\n",
       "      <td>3.223899</td>\n",
       "      <td>5.834154</td>\n",
       "      <td>4.742391</td>\n",
       "      <td>6.827058</td>\n",
       "      <td>7.894844</td>\n",
       "      <td>7.209535</td>\n",
       "      <td>8.370075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_eq_4</th>\n",
       "      <td>3.501302</td>\n",
       "      <td>6.555578</td>\n",
       "      <td>5.287484</td>\n",
       "      <td>7.713828</td>\n",
       "      <td>9.044416</td>\n",
       "      <td>8.220869</td>\n",
       "      <td>9.765026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3</th>\n",
       "      <td>2.875835</td>\n",
       "      <td>5.296642</td>\n",
       "      <td>3.655122</td>\n",
       "      <td>5.827752</td>\n",
       "      <td>6.117100</td>\n",
       "      <td>6.102522</td>\n",
       "      <td>5.785744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_3.5</th>\n",
       "      <td>3.141460</td>\n",
       "      <td>6.048718</td>\n",
       "      <td>4.109852</td>\n",
       "      <td>6.695766</td>\n",
       "      <td>7.161977</td>\n",
       "      <td>7.097910</td>\n",
       "      <td>6.982466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_peer_4</th>\n",
       "      <td>3.407086</td>\n",
       "      <td>6.800794</td>\n",
       "      <td>4.564582</td>\n",
       "      <td>7.563779</td>\n",
       "      <td>8.206854</td>\n",
       "      <td>8.093298</td>\n",
       "      <td>8.179187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_3</th>\n",
       "      <td>3.116898</td>\n",
       "      <td>4.408355</td>\n",
       "      <td>3.840304</td>\n",
       "      <td>4.983875</td>\n",
       "      <td>5.436244</td>\n",
       "      <td>5.024522</td>\n",
       "      <td>5.129680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_3.5</th>\n",
       "      <td>3.422701</td>\n",
       "      <td>5.012383</td>\n",
       "      <td>4.325897</td>\n",
       "      <td>5.711243</td>\n",
       "      <td>6.367644</td>\n",
       "      <td>5.840244</td>\n",
       "      <td>6.217058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iRet_erc_4</th>\n",
       "      <td>3.728504</td>\n",
       "      <td>5.616411</td>\n",
       "      <td>4.811491</td>\n",
       "      <td>6.438610</td>\n",
       "      <td>7.299045</td>\n",
       "      <td>6.655965</td>\n",
       "      <td>7.304435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>4.770000</td>\n",
       "      <td>7.210000</td>\n",
       "      <td>8.060000</td>\n",
       "      <td>7.070000</td>\n",
       "      <td>8.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMA_active</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>5.520000</td>\n",
       "      <td>8.210000</td>\n",
       "      <td>9.060000</td>\n",
       "      <td>8.070000</td>\n",
       "      <td>9.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>post_ExpRet</th>\n",
       "      <td>5.070730</td>\n",
       "      <td>7.424359</td>\n",
       "      <td>4.785615</td>\n",
       "      <td>7.433425</td>\n",
       "      <td>8.097552</td>\n",
       "      <td>7.575155</td>\n",
       "      <td>7.983203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  US_RE     US_PE     US_HY     SP500  Rusell2000      EAFE  \\\n",
       "iRet_eq_3      2.946497  5.112730  4.197299  5.940288    6.745272  6.198200   \n",
       "iRet_eq_3.5    3.223899  5.834154  4.742391  6.827058    7.894844  7.209535   \n",
       "iRet_eq_4      3.501302  6.555578  5.287484  7.713828    9.044416  8.220869   \n",
       "iRet_peer_3    2.875835  5.296642  3.655122  5.827752    6.117100  6.102522   \n",
       "iRet_peer_3.5  3.141460  6.048718  4.109852  6.695766    7.161977  7.097910   \n",
       "iRet_peer_4    3.407086  6.800794  4.564582  7.563779    8.206854  8.093298   \n",
       "iRet_erc_3     3.116898  4.408355  3.840304  4.983875    5.436244  5.024522   \n",
       "iRet_erc_3.5   3.422701  5.012383  4.325897  5.711243    6.367644  5.840244   \n",
       "iRet_erc_4     3.728504  5.616411  4.811491  6.438610    7.299045  6.655965   \n",
       "CMA            7.000000  8.800000  4.770000  7.210000    8.060000  7.070000   \n",
       "CMA_active     7.000000  8.800000  5.520000  8.210000    9.060000  8.070000   \n",
       "post_ExpRet    5.070730  7.424359  4.785615  7.433425    8.097552  7.575155   \n",
       "\n",
       "                     EM  \n",
       "iRet_eq_3      6.975123  \n",
       "iRet_eq_3.5    8.370075  \n",
       "iRet_eq_4      9.765026  \n",
       "iRet_peer_3    5.785744  \n",
       "iRet_peer_3.5  6.982466  \n",
       "iRet_peer_4    8.179187  \n",
       "iRet_erc_3     5.129680  \n",
       "iRet_erc_3.5   6.217058  \n",
       "iRet_erc_4     7.304435  \n",
       "CMA            8.030000  \n",
       "CMA_active     9.030000  \n",
       "post_ExpRet    7.983203  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpRet_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ew</th>\n",
       "      <th>peer</th>\n",
       "      <th>erc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>0.005548</td>\n",
       "      <td>0.005313</td>\n",
       "      <td>0.006116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>0.014428</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.012081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>0.010902</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.009712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.017735</td>\n",
       "      <td>0.017360</td>\n",
       "      <td>0.014547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>0.022991</td>\n",
       "      <td>0.020898</td>\n",
       "      <td>0.018628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>0.020227</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.016314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.027899</td>\n",
       "      <td>0.023934</td>\n",
       "      <td>0.021748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ew      peer       erc\n",
       "US_RE       0.005548  0.005313  0.006116\n",
       "US_PE       0.014428  0.015042  0.012081\n",
       "US_HY       0.010902  0.009095  0.009712\n",
       "SP500       0.017735  0.017360  0.014547\n",
       "Rusell2000  0.022991  0.020898  0.018628\n",
       "EAFE        0.020227  0.019908  0.016314\n",
       "EM          0.027899  0.023934  0.021748"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.dot(LW_cov, portf_weight.loc[['w_eq', 'w_peer', 'w_erc']].T), index=LW_cov.index, columns= ['ew', 'peer', 'erc'])*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081116</td>\n",
       "      <td>0.083128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.081116  0.083128"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_w= portf_weight_final.loc[[ 'w_aRUO_3.5unc', 'w_aRUO_3.5']]\n",
    "\n",
    "tmp_ret= np.dot(tmp_w, ExpRet_final.loc['CMA_active'])/100\n",
    "tmp_vol= np.sqrt(np.diag(np.dot(np.dot(tmp_w, LW_cov_active), tmp_w.T)))*2\n",
    "pd.DataFrame(tmp_ret).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.109126</td>\n",
       "      <td>0.120361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.109126  0.120361"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tmp_vol).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05729934,  0.05415444])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ret- 4/2* tmp_vol**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6\n",
       "0  1.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "2  0.0  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "3  0.0  0.0  0.0  1.0  0.0  0.0  0.0\n",
       "4  0.0  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "5  0.0  0.0  0.0  0.0  0.0  1.0  0.0\n",
       "6  0.0  0.0  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(view_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CMA_active</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>8.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>9.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>8.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>9.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CMA_active\n",
       "US_RE             7.00\n",
       "US_PE             8.80\n",
       "US_HY             5.52\n",
       "SP500             8.21\n",
       "Rusell2000        9.06\n",
       "EAFE              8.07\n",
       "EM                9.03"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(ExpRet_final.loc['CMA_active'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_BL_unc</th>\n",
       "      <th>w_BL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US_RE</th>\n",
       "      <td>0.348520</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_PE</th>\n",
       "      <td>0.338258</td>\n",
       "      <td>0.388598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_HY</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SP500</th>\n",
       "      <td>0.113383</td>\n",
       "      <td>0.142746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rusell2000</th>\n",
       "      <td>0.040483</td>\n",
       "      <td>0.033797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EAFE</th>\n",
       "      <td>0.094314</td>\n",
       "      <td>0.100226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EM</th>\n",
       "      <td>0.065042</td>\n",
       "      <td>0.056891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            w_BL_unc      w_BL\n",
       "US_RE       0.348520  0.250000\n",
       "US_PE       0.338258  0.388598\n",
       "US_HY       0.000000  0.027743\n",
       "SP500       0.113383  0.142746\n",
       "Rusell2000  0.040483  0.033797\n",
       "EAFE        0.094314  0.100226\n",
       "EM          0.065042  0.056891"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl_w= portf_weight_final.loc[['w_BL_unc', 'w_BL']]\n",
    "bl_ret= ExpRet_final.loc['post_ExpRet'].values/100\n",
    "bl_w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.066829</td>\n",
       "      <td>0.068337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110601</td>\n",
       "      <td>0.115428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.066829  0.068337\n",
       "0  0.110601  0.115428"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ret_BL=pd.DataFrame(np.dot(bl_w, bl_ret)).T\n",
    "tmp_vol_BL= pd.DataFrame(np.sqrt(np.diag(np.dot(np.dot(bl_w, LW_cov_active+ post_cov), bl_w.T)))*2).T\n",
    "\n",
    "pd.concat( [tmp_ret_BL, tmp_vol_BL], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
